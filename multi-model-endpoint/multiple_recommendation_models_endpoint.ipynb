{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using XGBoost\n",
    "_**Hosting multiple trained machine learning models on a single Amazon SageMaker Endpoint**_\n",
    "\n",
    "This notebook demonstrates\n",
    "\n",
    "* Hosting multiple trained machine learning models on a single Amazon SageMaker endpoint\n",
    "* Directing inference traffic to the endpoint and to a specific model.\n",
    "\n",
    "\n",
    "**Table of Contents** \n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Section 1 - Setup](#setup)\n",
    "3. [Section 2 - Create the multi-model endpoint](#create-endpoint)\n",
    "4. [Section 3 - Execute movie recommedation predictions](#movie-predictions)\n",
    "5. [Section 4 - Update the multi-model endpoint with second recommendation model](#update-endpoint)\n",
    "6. [Section 5 - Execute music recommedation predictions](#music-predictions)\n",
    "8. [Clean up](#cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='intro'></a>\n",
    "\n",
    "Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. \n",
    "Amazon SageMaker is a fully-managed service that covers the entire machine learning workflow. You can label and \n",
    "prepare your data, choose an algorithm, train a model, and then tune and optimize it for deployment. Amazon SageMaker \n",
    "gets your models into production to make predictions or take actions with less effort and lower costs than was \n",
    "previously possible.\n",
    "\n",
    "With Amazon SageMaker Multi-Model Endpoints, you can create an endpoint that hosts multiple models. These Endpoints are well suited to cases where there are a large number of models that can be served from a shared inference container and when the prediction request tolerates occasional cold start latency penalties for invoking infrequently used models.\n",
    "\n",
    "At high level, Amazon SageMaker manages the lifetime of the models in-memory for multi-model endpoints. When an invocation request is made for a particular model, Amazon SageMaker routes the request to a particular instance, downloads the model from S3 to that instance, and loads the required model to the memory of the container. Then Amazon SageMaker performs an invocation on the model. If the model is already loaded in memory, the invocation will be fast since the downloading and loading steps are skipped.\n",
    "\n",
    "To demonstrate how multi-model endpoints are created, updated and used, this notebook provides an example using two XGBoost models, one for movie recommendations and one for music recommendations. The multi-model endpoint capability is designed to work across all machine learning frameworks and algorithms including those where you bring your own container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Setup <a id='setup'></a>\n",
    "\n",
    "In this section, we will import the necessary libraries, setup variables and examine data that was used to train the XGBoost movie recommendation model provided with this notebook.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* The AWS region used to host your model.\n",
    "* The IAM role associated with this SageMaker notebook instance.\n",
    "* The S3 bucket used to store the data used to train your model, any additional model data, and the data captured from model invocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and register an XGBoost container that can serve multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU awscli boto3 sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inference container to serve multiple models in a multi-model endpoint, it must implement additional APIs in order to load, list, get, unload and invoke specific models.\n",
    "\n",
    "The 'mme' branch of the SageMaker XGBoost Container repository is an example implementation on how to adapt SageMaker's XGBoost framework container to use Multi Model Server, a framework that provides an HTTP frontend that implements the additional container APIs required by multi-model endpoints, and also provides a pluggable backend handler for serving models using a custom framework, in this case the XGBoost framework.\n",
    "\n",
    "Using this branch, below we will build an XGBoost container that fulfills all of the multi-model endpoint container requirements, and then upload that image to Amazon Elastic Container Registry (ECR). Because uploading the image to ECR may create a new ECR repository, this notebook requires permissions in addition to the regular SageMakerFullAccess permissions. The easiest way to add these permissions is simply to add the managed policy AmazonEC2ContainerRegistryFullAccess to the role that you used to start your notebook instance. There's no need to restart your notebook instance when you do this, the new permissions will be available immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM_NAME = 'multi-model-xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s $ALGORITHM_NAME\n",
    "\n",
    "algorithm_name=$1\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration\n",
    "region=$(aws configure get region)\n",
    "\n",
    "ecr_image=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email --registry-ids ${account})\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# First clear out any prior version of the cloned repo\n",
    "rm -rf sagemaker-xgboost-container/\n",
    "\n",
    "# Clone the xgboost container repo\n",
    "git clone --single-branch --branch mme https://github.com/aws/sagemaker-xgboost-container.git\n",
    "cd sagemaker-xgboost-container/\n",
    "\n",
    "# Build the \"base\" container image that encompasses the installation of the\n",
    "# XGBoost framework and all of the dependencies needed.\n",
    "docker build -q -t xgboost-container-base:0.90-2-cpu-py3 -f docker/0.90-2/base/Dockerfile.cpu .\n",
    "\n",
    "# Create the SageMaker XGBoost Container Python package.\n",
    "python setup.py bdist_wheel --universal\n",
    "\n",
    "# Build the \"final\" container image that encompasses the installation of the\n",
    "# code that implements the SageMaker multi-model container requirements.\n",
    "docker build -q -t ${algorithm_name} -f docker/0.90-2/final/Dockerfile.cpu .\n",
    "\n",
    "docker tag ${algorithm_name} ${ecr_image}\n",
    "\n",
    "docker push ${ecr_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "ACCOUNT_ID = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION     = boto3.Session().region_name\n",
    "BUCKET     = sagemaker_session.default_bucket()\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "XGB_CONTAINER = get_image_uri(REGION, 'xgboost', '0.90-1')\n",
    "\n",
    "DATA_PREFIX = 'sagemaker/Recommendations-MultiModelEndpoint'\n",
    "\n",
    "RECOMMENDATIONS_MODEL_NAME     = 'recommendations' \n",
    "MULTI_MODEL_ARTIFACTS  = 'multi_model_artifacts'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m4.xlarge'\n",
    "\n",
    "XGB_CONTAINER = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(ACCOUNT_ID, REGION, \n",
    "                                                                           ALGORITHM_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained models and data\n",
    "\n",
    "LOCAL_MODELS_DIR='../../models'\n",
    "LOCAL_DATA_DIR='../../data'\n",
    "\n",
    "MOVIE_RECOMMENDATION_MODEL='movie-rec-model.tar.gz'\n",
    "MUSIC_RECOMMENDATION_MODEL='music-rec-model.tar.gz'\n",
    "\n",
    "MOVIE_RECOMMENDATION_TEST_DATA='movielens_users_items_for_predictions.csv'\n",
    "\n",
    "MUSIC_RECOMMENDATION_TEST_DATA='music_users_items_for_predictions.csv'\n",
    "\n",
    "MOVIE_META_DATA='movie_metadata.csv'\n",
    "SONG_META_DATA='song_metadata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import models into hosting\n",
    "A big difference for multi-model endpoints is that when creating the Model entity, the container's `ModelDataUrl` is the S3 prefix where the model artifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when actually invoking the model. Remember to close the location with a trailing slash.\n",
    "\n",
    "The `Mode` of container is specified as `MultiModel` to signify that the container will host multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model artifacts to be found by the endpoint\n",
    "As described above, the multi-model endpoint is configured to find its model artifacts in a specific location in S3. For each trained model, we make a copy of its model artifacts into that location.\n",
    "\n",
    "In our example, we are storing all the models within a single folder. The implementation of multi-model endpoints is flexible enough to permit an arbitrary folder structure. For a set of housing models for example, you could have a top level folder for each region, and the model artifacts would be copied to those regional folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are purposely *not* copying the first model. This will be copied later in the notebook to demonstrate how to dynamically add new models to an already running endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy model to S3 bucket.\n",
    "def copy_model_to_s3(model_name):\n",
    "    key = os.path.join(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, model_name)\n",
    "    with open(LOCAL_MODELS_DIR+'/'+model_name, 'rb') as file_obj:\n",
    "        print(\"Uploading \", file_obj , \" to bucket \", BUCKET, \" as \" , key)\n",
    "        s3.Bucket(BUCKET).Object(key).upload_fileobj(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy movie recommendation model to S3\n",
    "copy_model_to_s3(MOVIE_RECOMMENDATION_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Amazon SageMaker model metadata\n",
    "Here we use `boto3` to establish the model metadata. Instead of describing a single model, this metadata will indicate the use of multi-model semantics and will identify the source location of all specific model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_model_metadata(multi_model_name, role):\n",
    "    # establish the place in S3 from which the endpoint will pull individual models\n",
    "    _model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "    _container = {\n",
    "        'Image':        XGB_CONTAINER,\n",
    "        'ModelDataUrl': _model_url,\n",
    "        'Mode':         'MultiModel'\n",
    "    }\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName = multi_model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        Containers = [_container])\n",
    "    \n",
    "    return _model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '{}-{}'.format(RECOMMENDATIONS_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "model_url = create_multi_model_metadata(name, role)\n",
    "print(\"model_url \", model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the multi-model endpoint\n",
    "There is nothing special about the SageMaker endpoint config metadata for a multi-model endpoint. You need to consider the appropriate instance type and number of instances for the projected prediction workload. The number and size of the individual models will drive memory requirements.\n",
    "\n",
    "Once the endpoint config is in place, the endpoint creation is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = name\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': ENDPOINT_INSTANCE_TYPE,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "endpoint_name = name\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes about 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print('    {}...'.format(status))\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise the multi-model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the boto3 interface above to create the endpoint config and endpoint, we use `RealTimePredictor` to get access to the endpoint for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import RealTimePredictor\n",
    "\n",
    "xgb_predictor = RealTimePredictor(endpoint_name)\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model url - \", model_url)\n",
    "print('Here are the models served by the endpoint :')\n",
    "!aws s3 ls $model_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 - Execute movie recommedation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv(LOCAL_DATA_DIR+\"/\"+MOVIE_META_DATA, delimiter ='|', encoding='latin-1')\n",
    "\n",
    "movie_df.columns = [\"movie id\", \"movie title\", \"release date\", \"video release date\",\n",
    "              \"IMDb URL\", \"unknown\", \"Action\", \"Adventure\", \"Animation\",\n",
    "              \"Children's\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Fantasy\",\n",
    "              \"Film-Noir\",\"Horror\",\"Musical\",\"Mystery\", \"Romance\",\"Sci-Fi\",\n",
    "              \"Thriller\",\"War\",\"Western\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "def get_recommendations_for_user(model_name,user_id, show_predictions):\n",
    "    predictions_for_user = str(user_id)\n",
    "    predictions = []\n",
    "    \n",
    "    with open(LOCAL_DATA_DIR+\"/\"+MOVIE_RECOMMENDATION_TEST_DATA, 'r') as f:\n",
    "        contents = f.readlines() \n",
    "    \n",
    "    for i in range(0, len(contents) - 1):\n",
    "        line = contents[i]\n",
    "        split_data = line.split(',')\n",
    "        #Remove the original rating value from data used for prediction\n",
    "        original_value = split_data.pop(0)\n",
    "        original_value = split_data.pop(0)\n",
    "        #print('original rating ', original_value)\n",
    "\n",
    "        user = split_data[0]\n",
    "        item = split_data[1]\n",
    "        #print('Predicting rating for User ', user, 'for item ', item)\n",
    "\n",
    "        if (user == predictions_for_user) : \n",
    "\n",
    "            payload = ','.join(split_data)\n",
    "\n",
    "            response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                                  ContentType='text/csv', \n",
    "                                                  TargetModel=model_name,\n",
    "                                                  Body=payload)\n",
    "            prediction = response['Body'].read().decode('utf-8')\n",
    "\n",
    "            predictions.append([item, prediction])\n",
    "\n",
    "            #print(\"Original Value \", original_value , \"Prediction : \", float(prediction))\n",
    "\n",
    "    if show_predictions:        \n",
    "        sorted_predcitions =    sorted(predictions, key = lambda x: x[1], reverse=True)     \n",
    "\n",
    "        ## Let's show only the top 10 recommendations\n",
    "        recommendations = sorted_predcitions[0:9]\n",
    "\n",
    "        print(\"Recommended movies for user with id : \", predictions_for_user)\n",
    "        for rec in recommendations: \n",
    "            #print(\"rec is \", type(rec))\n",
    "            movie_id = int(rec[0])\n",
    "            #print(\"recommended_movie_item \", movie_id)\n",
    "            movie_match = movie_df.loc[movie_df['movie id'] == movie_id]\n",
    "            movie_titile = movie_match['movie title'].values[0]\n",
    "            print(\"\\t\", movie_match['movie title'].values[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get movie recommendations for a couple of user.\n",
    "user_ids = [100, 235]\n",
    "\n",
    "for user_id in user_ids:\n",
    "    get_recommendations_for_user(MOVIE_RECOMMENDATION_MODEL,user_id, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4 - Update the multi-model endpoint with second recommendation model  <a id='update-endpoint'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamically deploy another model\n",
    "Here we demonstrate the power of dynamic loading of new models. We purposely did not copy the first model when deploying models earlier. Now we deploy an additional model and can immediately invoke it through the multi-model endpoint. As with the earlier models, the first invocation to the new model takes longer, as the endpoint takes time to download the model and load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy music recommendation model to S3\n",
    "copy_model_to_s3(MUSIC_RECOMMENDATION_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model url - \", model_url)\n",
    "print('Here are the models served by the endpoint :')\n",
    "!aws s3 ls $model_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5 - Execute music recommedation predictions <a id='music-predictions'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = pd.read_csv(LOCAL_DATA_DIR+\"/\"+SONG_META_DATA, delimiter =',', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_music_recommendations_for_user(model_name,user_id, show_predictions):\n",
    "    predictions_for_user = str(user_id)\n",
    "    predictions = []\n",
    "    \n",
    "    with open(LOCAL_DATA_DIR+\"/\"+MUSIC_RECOMMENDATION_TEST_DATA, 'r') as f:\n",
    "        contents = f.readlines() \n",
    "    \n",
    "    for i in range(0, len(contents) - 1):\n",
    "        line = contents[i]\n",
    "        \n",
    "        #print(\"line \", line)\n",
    "        split_data = line.split(',')\n",
    "        \n",
    "        #print(\"split_data \", split_data)\n",
    "        #Remove the original rating value from data used for prediction\n",
    "        original_value = split_data.pop(0)\n",
    "        \n",
    "        user = split_data[0]\n",
    "        item = split_data[1]\n",
    "        \n",
    "        #print('Predicting rating for User ', user, 'for item ', item)\n",
    "\n",
    "        if (user == predictions_for_user) : \n",
    "\n",
    "            payload = ','.join(split_data)\n",
    "\n",
    "            response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                                  ContentType='text/csv', \n",
    "                                                  TargetModel=model_name,\n",
    "                                                  Body=payload)\n",
    "            prediction = response['Body'].read().decode('utf-8')\n",
    "\n",
    "            predictions.append([item, prediction])\n",
    "\n",
    "            #print(\"Original Value \", original_value , \"Prediction : \", prediction)\n",
    "\n",
    "    if show_predictions:        \n",
    "        sorted_predcitions =    sorted(predictions, key = lambda x: x[1], reverse=True)     \n",
    "\n",
    "        ## Let's show only the top 10 recommendations\n",
    "        recommendations = sorted_predcitions[0:9]\n",
    "\n",
    "        print(\"Recommended songs for user with id : \", user)\n",
    "        for rec in recommendations: \n",
    "            #print(\"rec is \", type(rec))\n",
    "            song_id = float(rec[0])\n",
    "            #print(\"recommended_song_item \", song_id)\n",
    "            song_match = song_df.loc[song_df['short_song_id'] == song_id]\n",
    "            song_title = song_match['title'].values[0]\n",
    "            print(\"\\t\", song_title )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = '30544.0'\n",
    "\n",
    "get_music_recommendations_for_user(MUSIC_RECOMMENDATION_MODEL,user_id, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : See if we can show more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating a model\n",
    "To update a model, you would follow the same approach as above and add it as a new model. For example, if you have retrained the NewYork_NY.tar.gz model and wanted to start invoking it, you would upload the updated model artifacts behind the S3 prefix with a new name such as NewYork_NY_v2.tar.gz, and then change the TargetModel field to invoke NewYork_NY_v2.tar.gz instead of NewYork_NY.tar.gz. You do not want to overwrite the model artifacts in Amazon S3, because the old version of the model might still be loaded in the containers or on the storage volume of the instances on the endpoint. Invocations to the new model could then invoke the old version of the model.\n",
    "\n",
    "Alternatively, you could stop the endpoint and re-deploy a fresh set of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Clean up <a id='cleanup'></a>\n",
    "Here, to be sure we are not billed for endpoints we are no longer using, we clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shut down the endpoint\n",
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe delete model too\n",
    "xgb_predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
