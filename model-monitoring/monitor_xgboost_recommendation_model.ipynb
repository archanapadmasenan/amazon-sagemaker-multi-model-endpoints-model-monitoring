{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Model Monitoring Beta Data Capture\n",
    "_**Hosting a Model in Amazon SageMaker and Capturing Inference requests, results, and metadata**_\n",
    "\n",
    "*NOTE - THIS FEATURE IS CONFIDENTIAL AND SHARED UNDER NDA. THE FEATURE IS IN BETA AND SHOULD NOT BE USED FOR PRODUCTION ENDPOINTS. THIS FEATURE IS CURRENTLY IN DEVELOPMENT AND THE API SPECIFICATIONS MAY CHANGE*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## Background\n",
    "\n",
    "Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that covers the entire machine learning workflow. You can label and prepare your data, choose an algorithm, train a model, and then tune and optimize it for deployment. Amazon SageMaker gets your models into production to make predictions or take actions with less effort and lower costs than was previously possible.\n",
    "\n",
    "Amazon SageMaker is adding new capabilities that monitor ML models while in production and that detect deviations in data quality in comparison to a baseline dataset (e.g. training data set). They enable you to capture the metadata and the input and output for invocations of the models that you deploy with Amazon SageMaker. They also enable you to analyze the data and monitor its quality. \n",
    "\n",
    "This notebook shows you how to capture the model invocation data from an endpoint and then view that data in S3. Soon, we plan to provide an additional example that shows how to analyze the data collected. \n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* The AWS region used to host you model.\n",
    "* The IAM role ARN used to give learning and hosting access to your data. See the documentation for how to specify these.\n",
    "* The S3 bucket used to store the data used to train your model, any additional model data, and the data captured from model invocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the SageMaker Internal Model\n",
    "This step is required to enable the data capture feature for beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure add-model --service-model file://sagemaker-internal-model.json --service-name sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "from sagemaker import get_execution_role, session\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))\n",
    "\n",
    "# You can modify the bucket to be your own, but make sure the role you chose for this notebook\n",
    "# has s3:PutObject permissions. This is the bucket into which the data will be captured\n",
    "bucket =  session.Session(boto3.Session()).default_bucket()\n",
    "print(\"Recommendations Bucket: {}\".format(bucket))\n",
    "prefix = 'sagemaker/Recommendations-ModelMonitor'\n",
    "\n",
    "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(bucket,reports_prefix)\n",
    "code_prefix = '{}/code'.format(prefix)\n",
    "s3_code_preprocessor_uri = 's3://{}/{}/{}'.format(bucket,code_prefix, 'preprocessor.py')\n",
    "s3_code_postprocessor_uri = 's3://{}/{}/{}'.format(bucket,code_prefix, 'postprocessor.py')\n",
    "\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))\n",
    "print(\"Preproc Code path: {}\".format(s3_code_preprocessor_uri))\n",
    "print(\"Postproc Code path: {}\".format(s3_code_postprocessor_uri))\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "\n",
    "#PRETRAINED_MODELS_BUCKET_PREFIX=\"https://reinvent-sagemaker-deployment-2019.s3-us-west-2.amazonaws.com/recommendations\"\n",
    "\n",
    "LOCAL_MODELS_DIR='../../models'\n",
    "LOCAL_DATA_DIR='../../data'\n",
    "\n",
    "MOVIE_RECOMMENDATION_MODEL='movie-rec-all-features.tar.gz'\n",
    "MUSIC_RECOMMENDATION_MODEL='music-rec-kiran.tar.gz'\n",
    "\n",
    "MOVIE_RECOMMENDATION_TRAIN_DATA='movie_train.csv'\n",
    "\n",
    "MOVIE_RECOMMENDATION_TEST_DATA='movie_test.csv'\n",
    "MUSIC_RECOMMENDATION_TEST_DATA='music_test.csv'\n",
    "\n",
    "\n",
    "##if not pathlib.Path(LOCAL_MODELS_DIR + \"/\" + MOVIE_RECOMMENDATION_MODEL).exists():\n",
    "   ## !wget $PRETRAINED_MODELS_BUCKET_PREFIX/models/movie-recommendation-models/$MOVIE_RECOMMENDATION_MODEL -P $LOCAL_MODELS_DIR\n",
    "\n",
    "##if not pathlib.Path(LOCAL_MODELS_DIR + \"/\" + MUSIC_RECOMMENDATION_MODEL).exists():\n",
    "   ## !wget $PRETRAINED_MODELS_BUCKET_PREFIX/models/music-recommendation-models/$MUSIC_RECOMMENDATION_MODEL -P $LOCAL_MODELS_DIR\n",
    "\n",
    "##if not pathlib.Path(LOCAL_DATA_DIR + \"/\" + MOVIE_RECOMMENDATION_TRAIN_DATA).exists():\n",
    "    ##!wget $PRETRAINED_MODELS_BUCKET_PREFIX/data/movie-recommendations-data/$MOVIE_RECOMMENDATION_TRAIN_DATA -P $LOCAL_DATA_DIR\n",
    "\n",
    "##if not pathlib.Path(LOCAL_DATA_DIR + \"/\" + MOVIE_RECOMMENDATION_TEST_DATA).exists():\n",
    "   ## !wget $PRETRAINED_MODELS_BUCKET_PREFIX/data/movie-recommendations-data/$MOVIE_RECOMMENDATION_TEST_DATA -P $LOCAL_DATA_DIR\n",
    "\n",
    "##if not pathlib.Path(LOCAL_DATA_DIR + \"/\" + MUSIC_RECOMMENDATION_TEST_DATA).exists():\n",
    "   ## !wget $PRETRAINED_MODELS_BUCKET_PREFIX/data/music-recommendations-data/$MUSIC_RECOMMENDATION_TEST_DATA -P $LOCAL_DATA_DIR\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly test the notebook has the right permissions needed for the demo. We will put a simple test object into the S3 bucket we specified above. If this command fails, then the demo will not work as intended. You can fix this by updating the role associated with this notebook to have \"s3:PutObject\" permissions and try this validation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time execution of this cell is good enough---\n",
    "# let's go ahead and upload some test scripts\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\"test_upload/test.txt\").upload_file('test_data/upload-test-file.txt')\n",
    "print(\"Success! You are all set to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model on a SageMaker Endpoint\n",
    "\n",
    "### Upload the pre-trained model to S3\n",
    "\n",
    "This code uploads a pre-trained XGBoost model that is ready for you to deploy. You can also use your own pre-trained model in this step. If you already have a pretrained model in s3, you can add it instead by specifying its s3_key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy model to S3 bucket.\n",
    "def copy_model_to_s3(model_name):\n",
    "    key = prefix + \"/\" + model_name\n",
    "    with open(LOCAL_MODELS_DIR+'/'+model_name, 'rb') as file_obj:\n",
    "        print(\"Uploading \", file_obj , \" to bucket \",bucket, \" as \" , key)\n",
    "        boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy movie recommendation model to S3\n",
    "copy_model_to_s3(MOVIE_RECOMMENDATION_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART A: Enable capturing real-time inference data from SageMaker Endpoints\n",
    "Let's first create an Endpoint to showcase the data capture capability in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SageMaker Model entity\n",
    "\n",
    "This step creates an Amazon SageMaker model from the model file previously uploaded to S3. If you have already created an Amazon SageMaker model, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = \"MoviePredictions-EndpointDataCaptureModel-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "model_url = 'https://{}.s3-{}.amazonaws.com/{}/{}'.format(bucket, region, prefix,MOVIE_RECOMMENDATION_MODEL)\n",
    "\n",
    "print (model_url)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_url,\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint Configuration\n",
    "\n",
    "This step is required to deploy an Amazon SageMaker model on an endpoint. To enable data capture for monitoring the model data quality, you specify the new capture option called \"DataCaptureConfig\". You can capture the request payload, the response payload or both with this configuration. The data capture is supported at the endpoint configuration level and applies to all variants. The captured data is stored in a json format. If you are using your own Amazon SageMaker model, you still need to complete this step to create a new endpoint configuration. The comments highlight the new API parameters for data capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "data_capture_sub_folder = \"datacapture-xgboost-movie-recommendations\"\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_sub_folder)\n",
    "\n",
    "print(\"Capture path:\"+ s3_capture_upload_path)\n",
    "\n",
    "data_capture_configuration = {\n",
    "    \"EnableCapture\": True, # flag turns data capture on and off\n",
    "    \"InitialSamplingPercentage\": 90, # sampling rate to capture data. max is 100%\n",
    "    \"DestinationS3Uri\": s3_capture_upload_path, # s3 location where captured data is saved\n",
    "    \"CaptureOptions\": [\n",
    "        {\n",
    "            \"CaptureMode\": \"Output\" # The type of capture this option enables. Values can be: [Output/Input]\n",
    "        },\n",
    "        {\n",
    "            \"CaptureMode\": \"Input\" # The type of capture this option enables. Values can be: [Output/Input]\n",
    "        }\n",
    "    ],\n",
    "    \"CaptureContentTypeHeader\": {\n",
    "       \"CsvContentTypes\": [\"text/csv\"], # headers which should signal to decode the payload into CSV format \n",
    "       \"JsonContentTypes\": [\"application/json\"] # headers which should signal to decode the payload into JSON format \n",
    "     }\n",
    "}\n",
    "\n",
    "endpoint_config_name = 'XGBoost-MovieRec-DataCaptureEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m5.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'InitialVariantWeight':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTrafficVariant'\n",
    "    }],\n",
    "    DataCaptureConfig = data_capture_configuration) # This is where the new capture options are applied\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint\n",
    "This step uses the endpoint configuration specified above to create an endpoint. This takes a few minutes (approximately 9 minutes) to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'XGBoost-MovieRec-DataCaptureEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the Deployed Model\n",
    "\n",
    "You can now send data to this endpoint to get inferences in realtime. Because we have enabled the data capture in the previous steps, the request and response payload along with some additional metadata will be saved in the S3 location you have specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "#def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    " #   with open(file_name, 'r') as f:\n",
    "  #      for row in f:\n",
    "   #         payload = row.rstrip('\\n')\n",
    "    #        response = runtime_client.invoke_endpoint(EndpointName=ep_name,\n",
    "     #                                    ContentType='text/csv', \n",
    "      #                                    Body=payload)\n",
    "       #     print(response) # TODO: Print after decoding utf8\n",
    "        #    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#endpoint_name='demo-data-capture-endpoint-2019-11-11-00-34-05'\n",
    "#!head -5 test_data/test-dataset-input-cols.csv > test_data/test_sample.csv\n",
    "###NOT WORKING BECAUSE OF COL NUM MISTMATCH\n",
    "\n",
    "#!head -5 data/movie_test.csv > test_data/test_sample.csv\n",
    "\n",
    "#print(\"Sending test traffic to the endpoint... Please wait...\")\n",
    "#invoke_endpoint(endpoint_name, 'test_data/test_sample.csv', runtime_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##For data capturing we will just perform predictions with a subset of test data.\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "with open(LOCAL_DATA_DIR+\"/\"+MOVIE_RECOMMENDATION_TEST_DATA, 'r') as f:\n",
    "    contents = f.readlines()\n",
    "    \n",
    "for i in range(0, 30):\n",
    "    line = contents[i]\n",
    "    split_data = line.split(',')\n",
    "    #Remove the original rating value from data used for prediction\n",
    "    original_value = split_data.pop(0)\n",
    "    \n",
    "    payload = ','.join(split_data)\n",
    "    \n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                              ContentType='text/csv', \n",
    "                                              Body=payload)\n",
    "    prediction = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    print(\"Original Value \", original_value , \"Prediction : \", float(prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step invokes the endpoint with included sample data for ~2 minutes. Data is captured based on the sampling percentage specified. If your endpoint runs for a long time, the data from your endpoint will continue to be captured and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : Fix the test data so we can run this for the entire dataset\n",
    "#for i in range(0, 40):    \n",
    "    ## Process the test content \n",
    " #   line = contents[i]\n",
    "  #  split_data = line.split(',')\n",
    "   # original_value = split_data.pop(0)\n",
    "   # payload = ','.join(split_data)\n",
    "    \n",
    "    \n",
    "    #response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "     #                                         ContentType='text/csv',\n",
    "      #                                        Body=payload)\n",
    "                                           \n",
    "    #print(\"Original Value \", original_value , \"Prediction : \", float(response['Body'].read().decode('utf-8')) )\n",
    "    #time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Captured Data\n",
    "\n",
    "Now let's list the data capture files stored in S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the s3 path is:\n",
    "\n",
    "s3://bucket-name/endpoint-name/year/month/day/hour/variant-name/filename.jsonl\n",
    "\n",
    "**NOTE:** This path is subject to change during the beta period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=data_capture_sub_folder)\n",
    "print(\"data_capture_sub_folder : \" , data_capture_sub_folder)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's view the contents of a single capture file. Here you should see all the data captured in a json-line formatted file. For simplicity the code shows only the first few lines of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's view the contents of a single line. This follows the data capture naming convention that you provided during the Endpoint Config setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B: Model Monitoring - Baselining and continous monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constraint suggestion with baseline/training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our training dataset let's ask SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data. But first, let's upload the training dataset (if you already have it, you can directly point to it). In this case, we have the training dataset which was used for training the xgboost model packaged in this example for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over the training dataset to S3 (if you already have it in S3, you could reuse it)\n",
    "baseline_prefix = prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(bucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(bucket, baseline_results_prefix)\n",
    "print(baseline_data_uri)\n",
    "print(baseline_results_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = open(LOCAL_DATA_DIR+\"/\"+MOVIE_RECOMMENDATION_TRAIN_DATA, 'rb')\n",
    "s3_key = os.path.join(baseline_prefix, 'data', 'movie_train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_key).upload_fileobj(training_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a baselining job with training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "from processingjob_wrapper import ProcessingJob\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'MOVIE-REC-baseline-xgb-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "processing_job = ProcessingJob(sm_client, role).create(job_name, baseline_data_uri, baseline_results_uri)\n",
    "\n",
    "resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "status = resp['ProcessingJobStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='InProgress':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "    status = resp['ProcessingJobStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "if status=='Failed':\n",
    "    print(resp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))\n",
    "\n",
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : CAN WE SHOW THE WHOLE FILE?\n",
    "constraints_file = get_obj_body(baseline_results_prefix+'/constraints.json')\n",
    "print(constraints_file[:4000])\n",
    "##TODO : Why are the genres treated as fractional?  \n",
    "##TODO : Would making rating fractional makes more sense?\n",
    "##TODO : What are all available inferred_types available and can we show them all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : CAN WE SHOW THE WHOLE FILE?\n",
    "statistics_file = get_obj_body(baseline_results_prefix+'/statistics.json')\n",
    "print(statistics_file[:4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing collected data for data quality issues\n",
    "\n",
    "We have collected the data above, let's proceed to analyze and monitor the data with Monitoring Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a schedule\n",
    "Let's create a Monitoring schedule for the previously created Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first copy over some test scripts to the S3 bucket so that they can be used for pre and post processing\n",
    "## HOW SHOULD WE USE preprocessor /postprocessor FOR THE WORKSHOP??\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(code_prefix+\"/preprocessor.py\").upload_file('preprocessor.py')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(code_prefix+\"/postprocessor.py\").upload_file('postprocessor.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schedule_wrapper import MonitoringSchedule\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# MonitoringSchedule is just a python helper to hide the large CreateMonitoringSchedule input payload. You can find it\n",
    "# in scheduler_wrapper.py in this package\n",
    "\n",
    "ms = MonitoringSchedule(sm_client, role)\n",
    "mon_schedule_name = 'REC-xgb-movie-rec-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "schedule = ms.create(mon_schedule_name, endpoint_name, s3_report_path, \n",
    "                     record_preprocessor_source_uri=s3_code_preprocessor_uri, \n",
    "                     post_analytics_source_uri=s3_code_postprocessor_uri,\n",
    "                     baseline_statistics_uri=baseline_results_uri + '/statistics.json',\n",
    "                     baseline_constraints_uri=baseline_results_uri+ '/constraints.json')\n",
    "schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mon_schedule_name = ''\n",
    "desc_schedule_result = sm_client.describe_monitoring_schedule( MonitoringScheduleName=mon_schedule_name)\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start generating some artificial traffic\n",
    "The block below kicks off a thread to send some traffic to the created endpoint. Note that you need to stop the kernel to terminate this thread. Just having this here so that it can continue to generate traffic. If there is no traffic, the monitoring jobs will start to fail later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "## CAN WE USE DIFFERENT TEST CSVs to FIND DIFFERENT DRIFT\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "    \n",
    "    for i in range(0, 30):\n",
    "        line = contents[i]\n",
    "        split_data = line.split(',')\n",
    "        #Remove the original rating value from data used for prediction\n",
    "        original_value = split_data.pop(0)\n",
    "\n",
    "        payload = ','.join(split_data)\n",
    "\n",
    "        response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                                  ContentType='text/csv', \n",
    "                                                  Body=payload)\n",
    "        prediction = response['Body'].read().decode('utf-8')\n",
    "\n",
    "        #print(\"Original Value \", original_value , \"Prediction : \", float(prediction))\n",
    "    \n",
    "        time.sleep(1)\n",
    "            \n",
    "\n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint(endpoint_name, LOCAL_DATA_DIR+\"/\"+MOVIE_RECOMMENDATION_TEST_DATA, runtime_client)\n",
    "        \n",
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()\n",
    "\n",
    "# Note that you need to stop the kernel to stop the invocations\n",
    "##TODO : CAN WE INVOKE THIS TRAFFIC IN A SCRIPT, that can be kicked off in with the cloudformation script??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe and inspect the schedule\n",
    "Once you describe, see MonitoringScheduleStatus changes to Scheduled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mon_schedule_name = ''\n",
    "desc_schedule_result = sm_client.describe_monitoring_schedule( MonitoringScheduleName=mon_schedule_name)\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List executions\n",
    "Once the schedule is scheduled, it will kick of jobs at specified intervals. Here we are listing the latest 5 executions. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait till you cross the hour boundary (in UTC) to see executions kick off. The code below has the logic for waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : THIS CELL GAVE ME ERRORS FIRST COUPLE OF TIMES. FIX \n",
    "\n",
    "mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "latest_execution=None\n",
    "\n",
    "# Wait till an execution occurs\n",
    "while not mon_executions['MonitoringExecutionSummaries']:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "\n",
    "# if it is one exection, let's wait for it to reach terminal state\n",
    "if len(mon_executions['MonitoringExecutionSummaries']) == 1: \n",
    "    execution = mon_executions['MonitoringExecutionSummaries'][0]\n",
    "    while True:\n",
    "        if execution['ProcessingJobArn']:\n",
    "            job_name = execution['ProcessingJobArn'].split('/')[1]    \n",
    "            resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "            status = resp['ProcessingJobStatus']\n",
    "            print(\"Status: \" + status)\n",
    "            if status != 'InProgress':\n",
    "                break\n",
    "        time.sleep(60)\n",
    "    \n",
    "# now get the latest execution details    \n",
    "for execution_summary in mon_executions['MonitoringExecutionSummaries']:\n",
    "    print(\"ProcessingJob: {}\".format(execution_summary['ProcessingJobArn'].split('/')[1]))\n",
    "    print('MonitoringExecutionStatus: {} \\n'.format(execution_summary['MonitoringExecutionStatus']))\n",
    "    if not latest_execution:\n",
    "        exec_status = execution_summary['MonitoringExecutionStatus']\n",
    "        if  exec_status == 'Completed' or exec_status == 'Failed' or exec_status == 'CompletedWithViolations':\n",
    "            latest_execution = execution_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect a specific execution (latest execution here)¶\n",
    "In the previous cell, we picked up the latest Completed/Failed scheduled execution. Let's explore what went good or wrong. Here are the possible terminal states and what each of them mean:\n",
    "\n",
    "Completed - this means the monitoring execution completed and no issues were found in the violations report\n",
    "CompletedWithIssues - this means the execution completed, but constraint violations were detected\n",
    "Failed - the monitoring execution failed, may be due to client error (say role issues) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest_execution:\n",
    "    job_name=latest_execution['ProcessingJobArn'].split('/')[1]\n",
    "    job_status=latest_execution['MonitoringExecutionStatus']\n",
    "    desc_analytics_job_result=sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "    \n",
    "    if job_status == 'Completed' or job_status == 'CompletedWithViolations':\n",
    "        report_uri=desc_analytics_job_result['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "        print('Report Uri: {}'.format(report_uri))\n",
    "    else:\n",
    "        print('Job failed, todo: print failure reason and more details..')\n",
    "else:\n",
    "    print(\"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List the generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip('/')\n",
    "print('Report bucket: {}'.format(report_bucket))\n",
    "print('Report key: {}'.format(report_key))\n",
    "\n",
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Violations report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any violations compared to the baseline, it will be generated here. Let's list the violations.\n",
    "\n",
    "NOTE: The actual description is not user friendly now, we are working on rewording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "violationss_file = get_obj_body(report_key+'/constraint_violations.json')\n",
    "print(violationss_file[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other commands\n",
    "List, stop, start on the schedule object are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.list_monitoring_schedules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete the Resources\n",
    "\n",
    "You can keep your Endpoint running to continue capturing data. If you do not plan to collect more data or use this endpoint further, you should delete the endpoint to avoid incurring additional charges. Note that deleting your endpoint does not delete the data that was captured during the model invocaations. That data is persisted in S3 until you delete it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_monitoring_schedule(MonitoringScheduleName=mon_schedule_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
