{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using XGBoost\n",
    "With Amazon SageMaker Multi-Model Endpoints (new feature under NDA), customers can create an endpoint that hosts multiple models. These Endpoints are well suited to cases where there are a large number of models that can be served from a shared inference container and when the prediction request tolerates occasional cold start latency penalties for invoking infrequently used models.\n",
    "\n",
    "At a high level, Amazon SageMaker manages the lifetime of the models in-memory for multi-model endpoints. When an invocation request is made for a particular model, Amazon SageMaker routes the request to a particular instance, downloads the model from S3 to that instance, and loads the required model to the memory of the container. Then Amazon SageMaker performs an invocation on the model. If the model is already loaded in memory, the invocation will be fast since the downloading and loading steps are skipped.\n",
    "\n",
    "To demonstrate how multi-model endpoints are created and used, this notebook provides an example using a set of XGBoost models that each predict housing prices for a single location. The multi-model endpoint capability is designed to work across all machine learning frameworks and algorithms including those where you bring your own container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMPORARY FOR BETA: Get access to the new feature in boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure add-model --service-model file://sagemaker-2017-07-24.normal.json --service-name sagemaker-multimodel-endpoints\n",
    "!aws configure add-model --service-model file://sagemaker-runtime.normal.json --service-name sagemaker-runtime-multimodel-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker-multimodel-endpoints')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime-multimodel-endpoints')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "ACCOUNT_ID = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION     = boto3.Session().region_name\n",
    "BUCKET     = sagemaker_session.default_bucket()\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "XGB_CONTAINER = get_image_uri(REGION, 'xgboost', '0.90-1')\n",
    "# TEMP during beta: hard-code uri for beta container with multi-model endpoint support\n",
    "#XGB_CONTAINER = '878107166805.dkr.ecr.us-east-1.amazonaws.com/preprod-xgboost-framework:mms_beta_1'\n",
    "XGB_CONTAINER = '878107166805.dkr.ecr.us-west-2.amazonaws.com/preprod-xgboost-framework:mms_beta_1'\n",
    "\n",
    "DATA_PREFIX            = 'DEMO_MME_REC'\n",
    "RECOMMENDATIONS_MODEL_NAME     = 'recommendations' ##TODO : CHANGE THIS \n",
    "MULTI_MODEL_ARTIFACTS  = 'multi_model_artifacts'\n",
    "\n",
    "#TRAIN_INSTANCE_TYPE    = 'ml.m4.xlarge'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m4.xlarge'\n",
    "\n",
    "MOVIE_RECOMMENDATION_MODEL='movie-rec-all-features.tar.gz'\n",
    "MUSIC_RECOMMENDATION_MODEL='music-rec-kiran.tar.gz'\n",
    "\n",
    "MOVIE_RECOMMENDATION_TEST_DATA='movie_test.csv'\n",
    "MUSIC_RECOMMENDATION_TEST_DATA='music_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import models into hosting\n",
    "A big difference for multi-model endpoints is that when creating the Model entity, the container's `ModelDataUrl` is the S3 prefix where the model artifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when actually invoking the model. Remember to close the location with a trailing slash.\n",
    "\n",
    "The `Mode` of container is specified as `MultiModel` to signify that the container will host multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model artifacts to be found by the endpoint\n",
    "As described above, the multi-model endpoint is configured to find its model artifacts in a specific location in S3. For each trained model, we make a copy of its model artifacts into that location.\n",
    "\n",
    "In our example, we are storing all the models within a single folder. The implementation of multi-model endpoints is flexible enough to permit an arbitrary folder structure. For a set of housing models for example, you could have a top level folder for each region, and the model artifacts would be copied to those regional folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are purposely *not* copying the first model. This will be copied later in the notebook to demonstrate how to dynamically add new models to an already running endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get the recommendation models and test data from S3 bucket\n",
    "pretrained_models_bucket_prefix=\"https://reinvent-sagemaker-deployment-2019.s3-us-west-2.amazonaws.com/recommendations\"\n",
    "\n",
    "\n",
    "!wget $pretrained_models_bucket_prefix/models/movie-recommendation-models/$MOVIE_RECOMMENDATION_MODEL -P models\n",
    "!wget $pretrained_models_bucket_prefix/models/music-recommendation-models/$MUSIC_RECOMMENDATION_MODEL -P models\n",
    "\n",
    "!wget $pretrained_models_bucket_prefix/data/movie-recommendations-data/$MOVIE_RECOMMENDATION_TEST_DATA -P data\n",
    "!wget $pretrained_models_bucket_prefix/data/music-recommendations-data/$MUSIC_RECOMMENDATION_TEST_DATA -P data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy model to S3 bucket.\n",
    "def copy_model_to_s3(model_name):\n",
    "    key = os.path.join(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, model_name)\n",
    "    with open('models/'+model_name, 'rb') as file_obj:\n",
    "        print(\"Uploading \", file_obj , \" to bucket \", BUCKET, \" as \" , key)\n",
    "        s3.Bucket(BUCKET).Object(key).upload_fileobj(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy movie recommendation model to S3\n",
    "copy_model_to_s3(MOVIE_RECOMMENDATION_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Amazon SageMaker model metadata\n",
    "Here we use `boto3` to establish the model metadata. Instead of describing a single model, this metadata will indicate the use of multi-model semantics and will identify the source location of all specific model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_model_metadata(multi_model_name, role):\n",
    "    # establish the place in S3 from which the endpoint will pull individual models\n",
    "    _model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "    _container = {\n",
    "        'Image':        XGB_CONTAINER,\n",
    "        'ModelDataUrl': _model_url,\n",
    "        'Mode':         'MultiModel'\n",
    "    }\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName = multi_model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        Containers = [_container])\n",
    "    \n",
    "    return _model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '{}-{}'.format(RECOMMENDATIONS_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "model_url = create_multi_model_metadata(name, role)\n",
    "print(\"model_url \", model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the multi-model endpoint\n",
    "There is nothing special about the SageMaker endpoint config metadata for a multi-model endpoint. You need to consider the appropriate instance type and number of instances for the projected prediction workload. The number and size of the individual models will drive memory requirements.\n",
    "\n",
    "Once the endpoint config is in place, the endpoint creation is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = name\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': ENDPOINT_INSTANCE_TYPE,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "endpoint_name = name\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print('    {}...'.format(status))\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise the multi-model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the boto3 interface above to create the endpoint config and endpoint, we use `RealTimePredictor` to get access to the endpoint for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import RealTimePredictor\n",
    "\n",
    "xgb_predictor = RealTimePredictor(endpoint_name)\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model url - \", model_url)\n",
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get recommendations from test data\n",
    "def get_recommendations(model_name, test_data_file_name):\n",
    "    \n",
    "    print(\"Getting recommendations for test data in \", test_data_file_name, \" using the model \", model_name)\n",
    "    \n",
    "    with open('data/'+test_data_file_name, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "        \n",
    "    for i in range(0, 20):\n",
    "\n",
    "        line = contents[i]\n",
    "        #print(line)\n",
    "        split_data = line.split(',')\n",
    "        original_value = split_data.pop(0)\n",
    "\n",
    "        payload = ','.join(split_data)\n",
    "\n",
    "        response = runtime_sm_client.invoke_endpoint(\n",
    "                            EndpointName=endpoint_name,\n",
    "                            ContentType='text/csv',\n",
    "                            TargetModel=model_name,\n",
    "                            Body=payload)\n",
    "\n",
    "        prediction = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "        print(\"Original Value \", original_value , \"Prediction : \", prediction)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get movie recommendations\n",
    "get_recommendations(MOVIE_RECOMMENDATION_MODEL,MOVIE_RECOMMENDATION_TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : Need to show recommended movie names to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically deploy another model\n",
    "Here we demonstrate the power of dynamic loading of new models. We purposely did not copy the first model when deploying models earlier. Now we deploy an additional model and can immediately invoke it through the multi-model endpoint. As with the earlier models, the first invocation to the new model takes longer, as the endpoint takes time to download the model and load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy music recommendation model to S3\n",
    "copy_model_to_s3(MUSIC_RECOMMENDATION_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model url - \", model_url)\n",
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get music recommendations\n",
    "get_recommendations(MUSIC_RECOMMENDATION_MODEL,MUSIC_RECOMMENDATION_TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : Need to show recommended song names to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Here, to be sure we are not billed for endpoints we are no longer using, we clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shut down the endpoint\n",
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe delete model too\n",
    "xgb_predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
