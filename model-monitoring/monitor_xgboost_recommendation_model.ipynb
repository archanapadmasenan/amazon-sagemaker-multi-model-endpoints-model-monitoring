{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Models - Data Capture and Monitoring \n",
    "_**Hosting a Model in Amazon SageMaker and Capturing Inference requests, results, and metadata**_\n",
    "\n",
    "This notebook demonstrates \n",
    "* Hosting a trained machine learning model on Amazon SageMaker and capturing inference requests, results, and metadata\n",
    "* Analyzing a baseline(training) dataset to generate a baseline constraints for the model\n",
    "* Monitoring  live endpoint for violations against the baseline constraints. \n",
    "\n",
    "Table of Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Section 1 - Setup](#setup)\n",
    "3. [Section 2 - Deploy pre-trained model with model data capture enabled](#deploy)\n",
    "4. [Section 3 - Run predictions and analyze data captured](#analyze-data-captured)\n",
    "5. [Section 4 - Generate baseline statistics and constraints](#generate-baseline)\n",
    "6. [Section 5 - Monitor model and analyze data drift](#analyze-data-drift)\n",
    "7. [Section 6 - Retrigger Model Training](#retrigger-training)\n",
    "8. [Clean up](#cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that covers the entire machine learning workflow. You can label and prepare your data, choose an algorithm, train a model, and then tune and optimize it for deployment. Amazon SageMaker gets your models into production to make predictions or take actions with less effort and lower costs than was previously possible.\n",
    "\n",
    "Amazon SageMaker also provides capabilities that monitor ML models while in production and detect deviations in data quality in comparison to a baseline dataset (e.g. training data set). They enable you to capture the metadata and the input and output for invocations of the models that you deploy with Amazon SageMaker. They also enable you to analyze the data and monitor its quality. \n",
    "\n",
    "This notebook shows you how to capture the model invocation data from an endpoint and then view that data in S3. \n",
    "\n",
    "This notebook walks you through deploying a pretrained XGBoost movie recommendation model to a Amazon SageMaker RealTime Endpoint, send inference traffic to the endpoint and capture the model invocation data in an S3 bucket. It further shows how to perform continous monitoring of the deployed model, detect data drift and automatically rtrigger model retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Setup <a id='setup'></a>\n",
    "\n",
    "In this section, we will import the necessary libraries, setup variables and examine data that was used to train the XGBoost movie recommendation model provided with this notebook.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* The AWS region used to host you model.\n",
    "* The IAM role associated with this SageMaker notebook instance.\n",
    "* The S3 bucket used to store the data used to train your model, any additional model data, and the data captured from model invocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the SageMaker Internal Model\n",
    "This step is required to enable the data capture feature for beta.\n",
    "<font color='red'>TODO : NEED TO REMOVE THIS AFTER GA</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure add-model --service-model file://sagemaker-internal-model.json --service-name sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "from sagemaker import get_execution_role, session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 AWS region and  IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 S3 bucket and prefixes to hold captured data from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(bucket,reports_prefix)\n",
    "code_prefix = '{}/code'.format(prefix)\n",
    "s3_code_preprocessor_uri = 's3://{}/{}/{}'.format(bucket,code_prefix, 'preprocessor.py')\n",
    "s3_code_postprocessor_uri = 's3://{}/{}/{}'.format(bucket,code_prefix, 'postprocessor.py')\n",
    "\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))\n",
    "print(\"Preproc Code path: {}\".format(s3_code_preprocessor_uri))\n",
    "print(\"Postproc Code path: {}\".format(s3_code_postprocessor_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Pretrained movie recommendation model, training used and test data to use for inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Variables to the pretrained models, baseline and test data\n",
    "\n",
    "LOCAL_MODELS_DIR='../../models'\n",
    "LOCAL_DATA_DIR='../../data'\n",
    "\n",
    "MOVIE_RECOMMENDATION_MODEL='model.tar.gz'\n",
    "MUSIC_RECOMMENDATION_MODEL='music-rec-kiran.tar.gz'\n",
    "\n",
    "MOVIE_RECOMMENDATION_TRAIN_DATA='movie_train_age.csv'\n",
    "\n",
    "MOVIE_RECOMMENDATION_TEST_DATA='movie_test_age.csv'\n",
    "MUSIC_RECOMMENDATION_TEST_DATA='music_test.csv'\n",
    "\n",
    "MOVIE_RECOMMENDATION_BASELINE_DATA='baseline.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color='red'> 1.5 TODO : DESCRIBE DATA AND FEATURES </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Test access to the S3 bucket\n",
    "Let's quickly verify that the notebook has the right permissions to access the S3 bucket specified above.\n",
    "We will put a simple test object into the S3 bucket.  If this command fais, the data capture and model monioring capabilities will not work from this notebook.  You can fix this by updating the role associated with this notebook instnace to have \"s3:PutObject\" permissions and try this validation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and upload a sample test script.\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\"test_upload/test.txt\").upload_file('test_data/upload-test-file.txt')\n",
    "print(\"Success! You are all set to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Deploy pre-trained movie recommendation model with model data capture enabled <a id='deploy'></a>\n",
    "\n",
    "In this section, we will upload the pretrained model to the S3 bucket, create Amazon Sagemaker Model, configure Amazon SageMaker RealTime Endpoint with 'DataCapture' enabled and finally create the realtime endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Upload the pre-trained model to S3\n",
    "\n",
    "This code uploads a pre-trained XGBoost movie recommendation model that is ready for you to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy model to S3 bucket.\n",
    "def copy_model_to_s3(model_name):\n",
    "    key = prefix + \"/\" + model_name\n",
    "    with open(LOCAL_MODELS_DIR+'/'+model_name, 'rb') as file_obj:\n",
    "        print(\"Uploading \", file_obj , \" to bucket \",bucket, \" as \" , key)\n",
    "        boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy movie recommendation model to S3\n",
    "copy_model_to_s3(MOVIE_RECOMMENDATION_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Create SageMaker Model entity\n",
    "\n",
    "This step creates an Amazon SageMaker model from the movie recommendations model file previously uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "##'<font color='red'> 246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3 </font>\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')\n",
    "\n",
    "model_name = \"MoviePredictions-EndpointDataCaptureModel-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "model_url = 'https://{}.s3-{}.amazonaws.com/{}/{}'.format(bucket, region, prefix,MOVIE_RECOMMENDATION_MODEL)\n",
    "\n",
    "print (model_url)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_url,\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Create Endpoint Configuration\n",
    "\n",
    "This step is required to deploy an Amazon SageMaker model on a realtime endpoint.\n",
    "\n",
    "To enable data capture for monitoring the model data quality, you specify the new capture option called \"DataCaptureConfig\". You can capture the request payload, the response payload or both with this configuration. Captured data is persisted in the S3 bucket location YOU specify and have complete control over.  This allows you to version this data, secure it with IAM policies, encryption according to your needs. \n",
    "\n",
    "The data capture is supported at the endpoint configuration level and applies to all variants. The captured data is stored in a json format. The comments in the cell belwo highlight the new API parameters for data capture.\n",
    "\n",
    "<font color='red'> Can be used independent of Model monitoring.   (TODO : Move this to later in the monitoring section) <font>\n",
    "\n",
    "Can be used for further labeling efforts.  <font color='red'>  (TODO : Move this to talking points in the deck) <font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "data_capture_sub_folder = \"datacapture-xgboost-movie-recommendations\"\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_sub_folder)\n",
    "\n",
    "print(\"Capture path:\"+ s3_capture_upload_path)\n",
    "\n",
    "data_capture_configuration = {\n",
    "    \"EnableCapture\": True, # flag turns data capture on and off\n",
    "    \"InitialSamplingPercentage\": 90, # sampling rate to capture data. max is 100%\n",
    "    \"DestinationS3Uri\": s3_capture_upload_path, # s3 location where captured data is saved\n",
    "    \"CaptureOptions\": [\n",
    "        {\n",
    "            \"CaptureMode\": \"Output\" # The type of capture this option enables. Values can be: [Output/Input]\n",
    "        },\n",
    "        {\n",
    "            \"CaptureMode\": \"Input\" # The type of capture this option enables. Values can be: [Output/Input]\n",
    "        }\n",
    "    ],\n",
    "    \"CaptureContentTypeHeader\": {\n",
    "       \"CsvContentTypes\": [\"text/csv\"], # headers which should signal to decode the payload into CSV format \n",
    "       \"JsonContentTypes\": [\"application/json\"] # headers which should signal to decode the payload into JSON format \n",
    "     }\n",
    "}\n",
    "\n",
    "endpoint_config_name = 'XGBoost-MovieRec-DataCaptureEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m5.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'InitialVariantWeight':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTrafficVariant'\n",
    "    }],\n",
    "    DataCaptureConfig = data_capture_configuration) # This is where the new capture options are applied\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Create Endpoint\n",
    "This step uses the endpoint configuration specified above to create an endpoint. This takes a few minutes (approximately 9 minutes) to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'XGBoost-SM-MovieRec-DataCaptureEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Execute movie predictions and analyze data captured from the deployed movie recommendation model <a id='analyze-data-captured'></a>\n",
    "\n",
    "In this section, we will invoke the enpoint created above using test data.  Since we already enabled 'datacapture' on the endpoint, the request payload and response along with additional metadata will be saved in the S3 location you have specified.  We will examine the data captured in the S3 bucket.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  Invoke the Deployed Model\n",
    "\n",
    "<font color='red'> TODO : Show the recommended movie names </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <font color='red'> Using these endpoint for testing to iterate quickly.  TODO : Delete in final version. </font>\n",
    "#endpoint_name='XGBoost-SM-MovieRec-DataCaptureEndpoint-2019-11-28-06-19-52'\n",
    "#arn:aws:sagemaker:us-west-2:555360056434:endpoint/xgboost-sm-movierec-datacaptureendpoint-2019-11-28-06-19-52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##For data capturing we will perform predictions with a subset of test data.\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "with open(LOCAL_DATA_DIR+\"/\"+MOVIE_RECOMMENDATION_TEST_DATA, 'r') as f:\n",
    "    contents = f.readlines()\n",
    "    \n",
    "for i in range(0, 30):\n",
    "    line = contents[i]\n",
    "    split_data = line.split(',')\n",
    "    #Remove the original rating value from data used for prediction\n",
    "    original_value = split_data.pop(0)\n",
    "    \n",
    "    payload = ','.join(split_data)\n",
    "    \n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                              ContentType='text/csv', \n",
    "                                              Body=payload)\n",
    "    prediction = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    print(\"Original Value \", original_value , \"Prediction : \", float(prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 View Captured Data : List files\n",
    "\n",
    "Now let's list the data capture files stored in S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the s3 path is:\n",
    "\n",
    "s3://bucket-name/endpoint-name/year/month/day/hour/variant-name/filename.jsonl\n",
    "\n",
    "**NOTE:** This path is subject to change during the beta period TODO : Delete this after GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=data_capture_sub_folder)\n",
    "print(\"data_capture_sub_folder : \" , data_capture_sub_folder)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 View Captured Data : View captured file content\n",
    "\n",
    "Next, let's view the contents of a single capture file. Here you should see all the data captured in a json-line formatted file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "\n",
    "print(capture_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 View Captured Data : View single line captured\n",
    "\n",
    "Finally, let's view the contents of a single line. This follows the data capture naming convention that you provided during the Endpoint Config setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 – Generate baseline statistics and constraints for continous model monitoring <a id='generate-baseline'></a>\n",
    "\n",
    "In this section, we will ask Amazon SageMaker to  suggest a set of baseline constraints and generate descriptive statistics to explain the data. We will review the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Upload baseline data to S3\n",
    "Training dataset which was used for training the xgboost movie recommendation model is provided with this notebook.  We will first upload this training dataset as baseline  to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over the training dataset to S3 \n",
    "baseline_prefix = prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(bucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(bucket, baseline_results_prefix)\n",
    "print(baseline_data_uri)\n",
    "print(baseline_results_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data_file = open(LOCAL_DATA_DIR+\"/\"+MOVIE_RECOMMENDATION_BASELINE_DATA, 'rb')\n",
    "s3_key = os.path.join(baseline_prefix, 'data', 'baseline.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_key).upload_fileobj(baseline_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Generate suggested constraints with baseline/training dataset using a Processing job\n",
    "\n",
    "Let's ask SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data, using the uploaded baseline dataset.  For this, we will create a Processing job\n",
    "\n",
    "(This step will take approximately 6 min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "from processingjob_wrapper import ProcessingJob\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'MOVIE-REC-baseline-xgb-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "processing_job = ProcessingJob(sm_client, role).create(job_name, baseline_data_uri, baseline_results_uri)\n",
    "\n",
    "resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "status = resp['ProcessingJobStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='InProgress':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "    status = resp['ProcessingJobStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "if status=='Failed':\n",
    "    print(resp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Explore the generated constraints and statistics\n",
    "\n",
    "<font color='red'> TODO : Render constraints and stats in tabular way.  Did this, but may be not showing all features?? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using this for quickly iterate : ProcessingJob MOVIE-REC-baseline-xgb-model-monitor-2019-11-28-06-29-43\n",
    "\n",
    "##ProcessingJob MOVIE-REC-baseline-xgb-model-monitor-2019-11-28-06-29-43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))\n",
    "\n",
    "## TODO : Same method defined multiple times.  Consolidate.\n",
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_file = get_obj_body(baseline_results_prefix+'/constraints.json')\n",
    "#print(constraints_file[:4000])\n",
    "print(constraints_file)\n",
    "##Point out the various inferred_type.  Fractional, Integral, String (ZipCode),\n",
    "##TODO : Why are the genres treated as fractional?  \n",
    "##TODO : Would making rating fractional makes more sense?\n",
    "##TODO : What are all available inferred_types available and can we show them all?\n",
    "##TODO : set age level constraint ?  Gender level constraint\n",
    "##TODO : ADD explanation of the constraints\n",
    "###TODO : Better rendering from the pysdk sample notebook.  Can we render as histograms as well??\n",
    "\n",
    "##Lets add details of these monitoring config settings.  Arun to forward the document.\n",
    "##Featurre level override is also available.\n",
    "\n",
    "##Base line 20 cols, but inference has 21 cols -- extra column check (what happens if we send 1 less column)\n",
    "## How about if one col is null ?? Completeness constraint.\n",
    "\n",
    "\n",
    "## Good to include in deck\n",
    "## Other Drift examples : loan applicaiton model.  loan approved/not; how much.  csutomers start using mobile and apply high amount.\n",
    "## could have caught in the drift.\n",
    "\n",
    "## DS retrained model based on a pivon in a single dimension.  now data formats coming in new inference.\n",
    "## new applicaiton inference traffic formats.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results examine the monitoring constraints.  Note that these constraints are suggestions from Amazon SageMaker.\n",
    "You can choose to override these values.  You can even update this file to include feature level constraint override."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : ADD explanation of the statistics\n",
    "statistics_file = get_obj_body(baseline_results_prefix+'/statistics.json')\n",
    "#print(statistics_file[:4000])\n",
    "print(statistics_file)\n",
    "\n",
    "##Talk about how the constraints can be modified.  Generated version is a suggestion.  you can accept as is or choose to modify according to your business needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Render statistics in a tabular format.\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "statistics_json = json.loads(statistics_file)\n",
    "schema_df = pd.io.json.json_normalize(statistics_json['features'])\n",
    "schema_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Render constraints in a tabular format.\n",
    "constraints_json = json.loads(constraints_file)\n",
    "schema_df = pd.io.json.json_normalize(constraints_json['features'])\n",
    "schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 – Monitor and analyze model for data drift <a id='analyze-data-drift'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have captured data based on inferences to the deployed realtime endpoint.  In this section, we will take this a step forward and configure continuous monitoring using 'Monitoring Schedules'.  We will further use results of various monitoring schedule executions to analyse inference data quality and detect data drift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Create a schedule\n",
    "Let's create a Monitoring schedule for the previously created Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first copy over some test scripts to the S3 bucket so that they can be used for pre and post processing\n",
    "## HOW SHOULD WE USE preprocessor /postprocessor FOR THE WORKSHOP??\n",
    "## IS This for baseline data? or inference data?\n",
    "\n",
    "## OPTIONAL /ADVANCED SECTION \n",
    "#multiple schedules\n",
    "#BYOC\n",
    "#infereance data capture --> should match the baseline\n",
    "#independent of the data capture\n",
    "#Prebuilt rules assume certain data structure/format.\n",
    "\n",
    "##TODO : WE NEED TO DELETE THIS?  ACTUALLY, MOVE TO AN ADVANCED SECTION\n",
    "\n",
    "## Pre processing -- \n",
    "## Post processing -- write additonal file as are report?\n",
    "#boto3.Session().resource('s3').Bucket(bucket).Object(code_prefix+\"/preprocessor.py\").upload_file('preprocessor.py')\n",
    "#boto3.Session().resource('s3').Bucket(bucket).Object(code_prefix+\"/postprocessor.py\").upload_file('postprocessor.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schedule_wrapper import MonitoringSchedule\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# MonitoringSchedule is just a python helper to hide the large CreateMonitoringSchedule input payload. You can find it\n",
    "# in scheduler_wrapper.py in this package\n",
    "\n",
    "ms = MonitoringSchedule(sm_client, role)\n",
    "mon_schedule_name = 'REC-SM-xgb-movie-rec-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "schedule = ms.create(mon_schedule_name, endpoint_name, s3_report_path, \n",
    "                     record_preprocessor_source_uri=s3_code_preprocessor_uri, \n",
    "                     post_analytics_source_uri=s3_code_postprocessor_uri,\n",
    "                     baseline_statistics_uri=baseline_results_uri + '/statistics.json',\n",
    "                     baseline_constraints_uri=baseline_results_uri+ '/constraints.json')\n",
    "schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = sm_client.describe_monitoring_schedule( MonitoringScheduleName=mon_schedule_name)\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Generate inference traffic \n",
    "\n",
    "Start generating some inference traffic. The block below kicks off a thread to send traffic to the created endpoint. Note that you need to stop the kernel to terminate this thread. Just having this here so that it can continue to generate traffic. If there is no traffic, the monitoring jobs will start to fail later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "## CAN WE USE DIFFERENT TEST CSVs to FIND DIFFERENT DRIFT\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "    \n",
    "    for i in range(0, 30):\n",
    "        line = contents[i]\n",
    "        split_data = line.split(',')\n",
    "        #Remove the original rating value from data used for prediction\n",
    "        original_value = split_data.pop(0)\n",
    "\n",
    "        payload = ','.join(split_data)\n",
    "\n",
    "        response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                                  ContentType='text/csv', \n",
    "                                                  Body=payload)\n",
    "        prediction = response['Body'].read().decode('utf-8')\n",
    "\n",
    "        #print(\"Original Value \", original_value , \"Prediction : \", float(prediction))\n",
    "    \n",
    "        time.sleep(1)\n",
    "            \n",
    "\n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint(endpoint_name, LOCAL_DATA_DIR+\"/\"+MOVIE_RECOMMENDATION_TEST_DATA, runtime_client)\n",
    "        \n",
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()\n",
    "\n",
    "# Note that you need to stop the kernel to stop the invocations\n",
    "##TODO : CAN WE INVOKE THIS TRAFFIC IN A SCRIPT, that can be kicked off in with the cloudformation script??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Describe and inspect the schedule.  \n",
    "Once you describe, see MonitoringScheduleStatus changes to Scheduled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = sm_client.describe_monitoring_schedule( MonitoringScheduleName=mon_schedule_name)\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 List executions \n",
    "List executions\n",
    "Once the schedule is scheduled, it will kick of jobs at specified intervals. Here we are listing the latest 5 executions. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait till you cross the hour boundary (in UTC) to see executions kick off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "mon_executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you may see the first few executions of the monitoring schedule fail due to data unavailability in the S3 bucket.  This is because the execution is looking for data captured in the previous hour.  \n",
    "\n",
    "Once the execution starts looking for the data in the current hour, you will start seeing execution completions.\n",
    "\n",
    "##TODO : For now copying files to the right folder.  But should fix this.\n",
    "\n",
    "Examples of drift.  Why is it important to monitor this?  What is the imapact.  -- Deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : Include results with special markdown here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 5.4 Wait for first execution of the monitoring schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Wait for first execution of the monitoring schedule\n",
    "\n",
    "First execution of the monitoring schedule will happen ON the hour mark.  This cell waits for the first execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "latest_execution=None ##TODO : Should we be capturing first execution instead of the latest??\n",
    "\n",
    "# Wait till an execution occurs\n",
    "while not mon_executions['MonitoringExecutionSummaries']:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Wait for completion of first monitoring schedule execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get the first execution -- THIS IS NOT WORKING SO WELL\n",
    "#execution = mon_executions['MonitoringExecutionSummaries'][0]\n",
    "\n",
    "#print(\"First execution of the monitoring schedule {}\".format(execution))\n",
    "\n",
    "#while True:\n",
    " #   execution_status = execution['MonitoringExecutionStatus']\n",
    "  #  print(\"Execution Status: {} ... \".format(execution_status))\n",
    "   # if execution_status != 'InProgress':\n",
    "    #    break\n",
    "    #time.sleep(60)    \n",
    "    \n",
    "    \n",
    " #   if execution['MonitoringExecutionStatus']:\n",
    "        \n",
    "  #  if execution['ProcessingJobArn']:\n",
    "   #     job_name = execution['ProcessingJobArn'].split('/')[1]    \n",
    "    #    resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "     #   status = resp['ProcessingJobStatus']\n",
    "      #  print(\"Execution Status: \" + status)\n",
    "       # if status != 'InProgress':\n",
    "        #    break\n",
    "        #time.sleep(60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.list_monitoring_schedules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "\n",
    "if len(mon_executions['MonitoringExecutionSummaries']) == 1: \n",
    "    execution = mon_executions['MonitoringExecutionSummaries'][0]\n",
    "    while True:\n",
    "        if execution['ProcessingJobArn']:\n",
    "            job_name = execution['ProcessingJobArn'].split('/')[1]    \n",
    "            resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "            status = resp['ProcessingJobStatus']\n",
    "            print(\"Processing Job Status: \" + status)\n",
    "            if status != 'InProgress':\n",
    "                break\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7 Get the latest execution details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : THIS CELL GAVE ME ERRORS FIRST COUPLE OF TIMES. FIX \n",
    "\n",
    "#mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "#latest_execution=None\n",
    "\n",
    "# Wait till an execution occurs\n",
    "#while not mon_executions['MonitoringExecutionSummaries']:\n",
    " #   print(\"Waiting for the 1st execution to happen...\")\n",
    "  #  time.sleep(60)\n",
    "   # mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "\n",
    "# if it is one exection, let's wait for it to reach terminal state\n",
    "#if len(mon_executions['MonitoringExecutionSummaries']) == 1: \n",
    " #   execution = mon_executions['MonitoringExecutionSummaries'][0]\n",
    "  #  while True:\n",
    "   #     if execution['ProcessingJobArn']:\n",
    "    #        job_name = execution['ProcessingJobArn'].split('/')[1]    \n",
    "     #       resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "      #      status = resp['ProcessingJobStatus']\n",
    "       #     print(\"Status: \" + status)\n",
    "        #    if status != 'InProgress':\n",
    "         #       break\n",
    "        #time.sleep(60)\n",
    "        \n",
    "#print(\"1\")        \n",
    "    \n",
    "# now get the latest execution details  \n",
    "mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "\n",
    "for execution_summary in mon_executions['MonitoringExecutionSummaries']:\n",
    "    print(\"ProcessingJob: {}\".format(execution_summary['ProcessingJobArn'].split('/')[1]))\n",
    "    print('MonitoringExecutionStatus: {} \\n'.format(execution_summary['MonitoringExecutionStatus']))\n",
    "    print(\"execution_summary is \" , execution_summary)\n",
    "    print(\"latest_executions is \" , latest_execution)    \n",
    "    if not latest_execution:\n",
    "        exec_status = execution_summary['MonitoringExecutionStatus']\n",
    "        print(\"exec_status is \" , exec_status)    \n",
    "        if  exec_status == 'Completed' or exec_status == 'Failed' or exec_status == 'CompletedWithViolations':\n",
    "            latest_execution = execution_summary\n",
    "            \n",
    "print(\"latest_executions is \" , latest_execution)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.8 Inspect a specific execution (latest execution here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, we picked up the latest Completed/Failed scheduled execution. Let's explore what went well or wrong. Here are the possible terminal states for monitoring schedule execution.\n",
    "\n",
    "* Completed - Monitoring execution is completed and no issues were found in the violations report\n",
    "\n",
    "* CompletedWithIssues - Monitoring execution is completed, but constraint violations were detected\n",
    "\n",
    "* Failed - Monitoring execution failed, may be due to client error (say role issues) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest_execution:\n",
    "    job_name=latest_execution['ProcessingJobArn'].split('/')[1]\n",
    "    job_status=latest_execution['MonitoringExecutionStatus']\n",
    "    desc_analytics_job_result=sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "    \n",
    "    if job_status == 'Completed' or job_status == 'CompletedWithViolations':\n",
    "        report_uri=desc_analytics_job_result['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "        print('Report Uri: {}'.format(report_uri))\n",
    "    else:\n",
    "        print('Job failed, todo: print failure reason and more details..')\n",
    "else:\n",
    "    print(\"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.9 List the generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip('/')\n",
    "print('Report bucket: {}'.format(report_bucket))\n",
    "print('Report key: {}'.format(report_key))\n",
    "\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.10 Violations report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any violations compared to the baseline, it will be generated here. Let's list the violations.\n",
    "\n",
    "<font color='red'> TODO : Add explanation to one or two features : 'Age' and 'Gender'? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "violations_file = get_obj_body(report_key+'/constraint_violations.json')\n",
    "print(violations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 - Retrigger Training <a id='retrigger-training'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Cloud Watch Alarms\n",
    "\n",
    "Now that we know how to capture the violations, we can go a step further and automatically trigger retraining of the model.  For this, we use cloud watch metrics and alarms\n",
    "\n",
    "<font color='red'> TODO : EXPLAIN.  What is the training data used for retraining?  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the SNS topics available.\n",
    "Make note of the topic starting 'SAGEMAKER-DEPLOYMENT-OPTIONS' (Note : This is match the cloudformation stack name used in prep.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sns')\n",
    "\n",
    "client.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_notifications_topic='ADD SNS TOPIC ARN HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Threshold for cloudwatch is independent of violations threshold.\n",
    "## Show the sequence of 0.1,0.2, but only trigger at 0.5\n",
    "##Email notification -- Talk about this.  Add to additions or optional section.\n",
    "\n",
    "cw_client = boto3.Session().client('cloudwatch')\n",
    "\n",
    "alarm_name='BASELINE_DRIFT_FEATURE_ALARM_AGAIN'\n",
    "alarm_desc='Trigger an cloudwatch alarm when the feature age drifts away from the baseline'\n",
    "feature_age_drift_threshold=0.1\n",
    "metric_name='feature_baseline_drift_Age'\n",
    "namespace='aws/sagemaker/Endpoints/data-metrics'\n",
    "\n",
    "endpoint_name=endpoint_name\n",
    "monitoring_schedule_name=mon_schedule_name\n",
    "\n",
    "cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    AlarmDescription=alarm_desc,\n",
    "    ActionsEnabled=True,\n",
    "    AlarmActions=[sns_notifications_topic],\n",
    "    MetricName=metric_name,\n",
    "    Namespace=namespace,\n",
    "    Statistic='Average',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'Endpoint',\n",
    "            'Value': endpoint_name\n",
    "        },\n",
    "        {\n",
    "            'Name': 'MonitoringSchedule',\n",
    "            'Value': monitoring_schedule_name\n",
    "        }\n",
    "    ],\n",
    "    Period=600,\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=feature_age_drift_threshold,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='breaching'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation - 1\n",
    "\n",
    "In a few minutes, you should see a cloudwatch alarm created.  This can be verified in the CloudWatch console\n",
    "\n",
    "\n",
    "![title](images/Cloudwatch_alarm.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation - 2\n",
    "\n",
    "In a few minutes, you should also see a new SageMaker model training job is triggered.  This can be verified in the SageMaker Console\n",
    "\n",
    "\n",
    "![title](images/ModelRetraining.png)\n",
    "\n",
    "<font color='red'> TODO : EXPLAIN.  What is the training data used for retraining?  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Cleanup : Delete the Resources <a id='cleanup'></a>\n",
    "\n",
    "You can keep your Endpoint running to continue capturing data. If you do not plan to collect more data or use this endpoint further, you should delete the endpoint to avoid incurring additional charges. Note that deleting your endpoint does not delete the data that was captured during the model invocaations. That data is persisted in S3 until you delete it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mon_schedule_name='REC-SM-xgb-movie-rec-model-monitor-schedule-2019-11-29-17-50-37'\n",
    "sm_client.delete_monitoring_schedule(MonitoringScheduleName=mon_schedule_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
