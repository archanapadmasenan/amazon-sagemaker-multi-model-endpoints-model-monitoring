{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using XGBoost\n",
    "With Amazon SageMaker Multi-Model Endpoints (new feature under NDA), customers can create an endpoint that hosts multiple models. These Endpoints are well suited to cases where there are a large number of models that can be served from a shared inference container and when the prediction request tolerates occasional cold start latency penalties for invoking infrequently used models.\n",
    "\n",
    "At a high level, Amazon SageMaker manages the lifetime of the models in-memory for multi-model endpoints. When an invocation request is made for a particular model, Amazon SageMaker routes the request to a particular instance, downloads the model from S3 to that instance, and loads the required model to the memory of the container. Then Amazon SageMaker performs an invocation on the model. If the model is already loaded in memory, the invocation will be fast since the downloading and loading steps are skipped.\n",
    "\n",
    "To demonstrate how multi-model endpoints are created and used, this notebook provides an example using a set of XGBoost models that each predict housing prices for a single location. The multi-model endpoint capability is designed to work across all machine learning frameworks and algorithms including those where you bring your own container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data for housing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMPORARY FOR BETA: Get access to the new feature in boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure add-model --service-model file://sagemaker-2017-07-24.normal.json --service-name sagemaker-multimodel-endpoints\n",
    "!aws configure add-model --service-model file://sagemaker-runtime.normal.json --service-name sagemaker-runtime-multimodel-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multiple house value prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker-multimodel-endpoints')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime-multimodel-endpoints')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "ACCOUNT_ID = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION     = boto3.Session().region_name\n",
    "BUCKET     = sagemaker_session.default_bucket()\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "XGB_CONTAINER = get_image_uri(REGION, 'xgboost', '0.90-1')\n",
    "# TEMP during beta: hard-code uri for beta container with multi-model endpoint support\n",
    "#XGB_CONTAINER = '878107166805.dkr.ecr.us-east-1.amazonaws.com/preprod-xgboost-framework:mms_beta_1'\n",
    "XGB_CONTAINER = '878107166805.dkr.ecr.us-west-2.amazonaws.com/preprod-xgboost-framework:mms_beta_1'\n",
    "\n",
    "DATA_PREFIX            = 'DEMO_MME_REC'\n",
    "HOUSING_MODEL_NAME     = 'recommendations' ##TODO : CHANGE THIS \n",
    "MULTI_MODEL_ARTIFACTS  = 'multi_model_artifacts'\n",
    "\n",
    "#TRAIN_INSTANCE_TYPE    = 'ml.m4.xlarge'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m4.xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split a given dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save datasets locally to support copying to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch a single training job for a given housing location\n",
    "There is nothing specific to multi-model endpoints in terms of the models it will host. They are trained in the same way as all other SageMaker models. Here we are using the XGBoost estimator and not waiting for the job to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kick off a model training job for each housing location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for all model training to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import models into hosting\n",
    "A big difference for multi-model endpoints is that when creating the Model entity, the container's `ModelDataUrl` is the S3 prefix where the model artifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when actually invoking the model. Remember to close the location with a trailing slash.\n",
    "\n",
    "The `Mode` of container is specified as `MultiModel` to signify that the container will host multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model artifacts to be found by the endpoint\n",
    "As described above, the multi-model endpoint is configured to find its model artifacts in a specific location in S3. For each trained model, we make a copy of its model artifacts into that location.\n",
    "\n",
    "In our example, we are storing all the models within a single folder. The implementation of multi-model endpoints is flexible enough to permit an arbitrary folder structure. For a set of housing models for example, you could have a top level folder for each region, and the model artifacts would be copied to those regional folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are purposely *not* copying the first model. This will be copied later in the notebook to demonstrate how to dynamically add new models to an already running endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'movie-rec-model.tar.gz', 'model-maybe-music.tar.gz'}\n",
    "\n",
    "for model in models:\n",
    "    key = os.path.join(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, model)\n",
    "    with open('models/'+model, 'rb') as file_obj:\n",
    "        print(\"Uploading \", file_obj , \" to bucket \", BUCKET, \" as \" , key)\n",
    "        s3.Bucket(BUCKET).Object(key).upload_fileobj(file_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Amazon SageMaker model metadata\n",
    "Here we use `boto3` to establish the model metadata. Instead of describing a single model, this metadata will indicate the use of multi-model semantics and will identify the source location of all specific model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_model_metadata(multi_model_name, role):\n",
    "    # establish the place in S3 from which the endpoint will pull individual models\n",
    "    _model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "    _container = {\n",
    "        'Image':        XGB_CONTAINER,\n",
    "        'ModelDataUrl': _model_url,\n",
    "        'Mode':         'MultiModel'\n",
    "    }\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName = multi_model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        Containers = [_container])\n",
    "    \n",
    "    return _model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '{}-{}'.format(HOUSING_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "model_url = create_multi_model_metadata(name, role)\n",
    "print(\"model_url \", model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the multi-model endpoint\n",
    "There is nothing special about the SageMaker endpoint config metadata for a multi-model endpoint. You need to consider the appropriate instance type and number of instances for the projected prediction workload. The number and size of the individual models will drive memory requirements.\n",
    "\n",
    "Once the endpoint config is in place, the endpoint creation is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name = '{}-{}'.format(\"Recommendations\", strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "\n",
    "endpoint_config_name = name\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': ENDPOINT_INSTANCE_TYPE,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "endpoint_name = name\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print('    {}...'.format(status))\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise the multi-model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the boto3 interface above to create the endpoint config and endpoint, we use `RealTimePredictor` to get access to the endpoint for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import RealTimePredictor\n",
    "\n",
    "xgb_predictor = RealTimePredictor(endpoint_name)\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke multiple individual models hosted behind a single endpoint\n",
    "Here we iterate through a set of housing predictions, choosing the specific location-based housing model at random. Notice the cold start price paid for the first invocation of any given model. Subsequent invocations of the same model take advantage of the model already being loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model url - \", model_url)\n",
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_name=\"movie-rec-model.tar.gz\"\n",
    "#full_model_name=\"model-maybe-music.tar.gz\"\n",
    "\n",
    "#payload='502,678,883702448,23092,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0'\n",
    "#payload='741,682,891455960,63108,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1'\n",
    "#payload='276,127,874786568,95064,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0'\n",
    "payload='574,347,891278860,53188,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0'\n",
    "#payload='542,194,886532534,60515,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0'\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "                        EndpointName=endpoint_name,\n",
    "                        ContentType='text/csv',\n",
    "                        TargetModel=full_model_name,\n",
    "                        Body=payload)\n",
    "\n",
    "#predicted_value = json.loads(response['Body'].read())[0]\n",
    "\n",
    "prediction = response['Body'].read().decode('utf-8')\n",
    "print(\"prediction : \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : Need to show predictions from the test data file.\n",
    "##Test data has already been prepped (in the XGBoost notebook) and uploaded here.\n",
    "test_data = pd.read_csv('data/movielens_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/movielens_test.csv', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "    \n",
    "#print('contents ', type(contents))\n",
    "\n",
    "#for i in range(0, len(contents)):\n",
    "for i in range(0, 20):\n",
    "\n",
    "    line = contents[i]\n",
    "    #print(line)\n",
    "    split_data = line.split(',')\n",
    "    #print('type of split_data : ',   type(split_data))\n",
    "    #print('split_data : ', split_data)\n",
    "\n",
    "    original_value = split_data.pop(0)\n",
    "    \n",
    "    payload = ','.join(split_data)\n",
    "    #print('payload_string : \\n')\n",
    "    #print(payload)\n",
    "\n",
    "    #print('original_value : \\n')\n",
    "    #print(original_value)\n",
    "    \n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "                        EndpointName=endpoint_name,\n",
    "                        ContentType='text/csv',\n",
    "                        TargetModel=full_model_name,\n",
    "                        Body=payload)\n",
    "    \n",
    "    prediction = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    #print(\"type of prediction : \", type(prediction))\n",
    "    #prediction = response['Body'].read().decode('utf-8')\n",
    "    #print(\"prediction : \", prediction)\n",
    "\n",
    "    print(\"Original Value \", original_value , \"Prediction : \", prediction)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : will need to fix this.\n",
    "\n",
    "# iterate through invocations with random inputs against a random model showing results and latency\n",
    "for i in range(10):\n",
    "    model_name = LOCATIONS[np.random.randint(1, len(LOCATIONS[:PARALLEL_TRAINING_JOBS]))]\n",
    "    full_model_name = '{}.tar.gz'.format(model_name)\n",
    "    predict_one_house_value(gen_random_house()[1:], full_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically deploy another model\n",
    "Here we demonstrate the power of dynamic loading of new models. We purposely did not copy the first model when deploying models earlier. Now we deploy an additional model and can immediately invoke it through the multi-model endpoint. As with the earlier models, the first invocation to the new model takes longer, as the endpoint takes time to download the model and load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add another model to the endpoint and exercise it\n",
    "##TODO : Show how to add another model\n",
    "#deploy_artifacts_to_mme(training_jobs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the newly deployed model\n",
    "Exercise the newly deployed model without the need for any endpoint update or restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : Show using multiple models\n",
    "\n",
    "#model_name = LOCATIONS[0]\n",
    "#full_model_name = '{}.tar.gz'.format(model_name)\n",
    "#for i in range(5):\n",
    " #   features = gen_random_house()\n",
    "  #  predict_one_house_value(gen_random_house()[1:], full_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Here, to be sure we are not billed for endpoints we are no longer using, we clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shut down the endpoint\n",
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe delete model too\n",
    "xgb_predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
