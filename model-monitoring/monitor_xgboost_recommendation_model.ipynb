{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Models - Data Capture and Monitoring \n",
    "_**Hosting a trained machine learning model in Amazon SageMaker & capturing inference requests, results, and metadata from the hosted model**_\n",
    "\n",
    "This notebook demonstrates \n",
    "* Hosting a trained machine learning model on Amazon SageMaker and capturing inference requests, results, and metadata\n",
    "* Analyzing a baseline(training) dataset to generate a baseline constraints for the model\n",
    "* Monitoring  live endpoint for violations against the baseline constraints. \n",
    "\n",
    "**Table of Contents** \n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Section 1 - Setup](#setup)\n",
    "3. [Section 2 - Deploy pre-trained model with model data capture enabled](#deploy)\n",
    "4. [Section 3 - Run predictions and analyze data captured](#analyze-data-captured)\n",
    "5. [Section 4 - Generate baseline statistics and constraints](#generate-baseline)\n",
    "6. [Section 5 - Monitor model and analyze data drift](#analyze-data-drift)\n",
    "7. [Section 6 - Retrigger Model Training](#retrigger-training)\n",
    "8. [Clean up](#cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that covers the entire machine learning workflow. You can label and prepare your data, choose an algorithm, train a model, and then tune and optimize it for deployment. Amazon SageMaker gets your models into production to make predictions or take actions with less effort and lower costs than was previously possible.\n",
    "\n",
    "Amazon SageMaker also provides capabilities that monitor ML models while in production and detect deviations in data quality in comparison to a baseline dataset (e.g. training data set). They enable you to capture the metadata and the input and output for invocations of the models that you deploy with Amazon SageMaker. They also enable you to analyze the data and monitor its quality. \n",
    "\n",
    "This notebook shows you how to capture the model invocation data from an endpoint and then view that data in S3. \n",
    "\n",
    "This notebook walks you through deploying a pretrained XGBoost movie recommendation model to a Amazon SageMaker RealTime Endpoint, send inference traffic to the endpoint and capture the model invocation data in an S3 bucket. It further shows how to perform continous monitoring of the deployed model, detect data drift and automatically rtrigger model retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Setup <a id='setup'></a>\n",
    "\n",
    "In this section, we will import the necessary libraries, setup variables and examine data that was used to train the XGBoost movie recommendation model provided with this notebook.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* The AWS region used to host your model.\n",
    "* The IAM role associated with this SageMaker notebook instance.\n",
    "* The S3 bucket used to store the data used to train your model, any additional model data, and the data captured from model invocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the SageMaker Internal Model\n",
    "This step is required to enable the data capture feature for beta.\n",
    "<font color='red'>TODO : NEED TO REMOVE THIS AFTER GA</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure add-model --service-model file://sagemaker-internal-model.json --service-name sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role, session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 AWS region and  IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 S3 bucket and prefixes to hold captured data from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets look for the S3 buckets avialable in this account.  And pick the bucket with name starting with \"deployment-options-workshopreinvent\"\n",
    "##Use this for the bucket name in the cell below\n",
    "s3_client = boto3.Session().client('s3')\n",
    "s3_client.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='deployment-options-workshopreinvent-a2f1b3e0'\n",
    "\n",
    "prefix = 'sagemaker/XGBoost-Recommendations'\n",
    "\n",
    "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(bucket,reports_prefix)\n",
    "code_prefix = '{}/code'.format(prefix)\n",
    "s3_code_preprocessor_uri = 's3://{}/{}/{}'.format(bucket,code_prefix, 'preprocessor.py')\n",
    "s3_code_postprocessor_uri = 's3://{}/{}/{}'.format(bucket,code_prefix, 'postprocessor.py')\n",
    "\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))\n",
    "print(\"Preproc Code path: {}\".format(s3_code_preprocessor_uri))\n",
    "print(\"Postproc Code path: {}\".format(s3_code_postprocessor_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Pretrained movie recommendation model, training used and test data to use for inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Variables for pretrained models, baseline and test data\n",
    "LOCAL_MODELS_DIR='../../models'\n",
    "LOCAL_DATA_DIR='../../data'\n",
    "\n",
    "MOVIE_RECOMMENDATION_MODEL='model-no-timestamp.tar.gz'\n",
    "MUSIC_RECOMMENDATION_MODEL='music-rec-kiran.tar.gz'\n",
    "\n",
    "MOVIE_RECOMMENDATION_TRAIN_DATA='movie_train_age.csv'\n",
    "\n",
    "MOVIE_RECOMMENDATION_TEST_DATA='movie_test.csv'\n",
    "MUSIC_RECOMMENDATION_TEST_DATA='music_test.csv'\n",
    "\n",
    "MOVIE_RECOMMENDATION_BASELINE_DATA='baseline.csv'\n",
    "\n",
    "MOVIE_DATA='u.item'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color='red'> 1.5 TODO : DESCRIBE DATA AND FEATURES </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Test access to the S3 bucket\n",
    "Let's quickly verify that the notebook has the right permissions to access the S3 bucket specified above.\n",
    "We will put a simple test object into the S3 bucket.  If this command fais, the data capture and model monioring capabilities will not work from this notebook.  You can fix this by updating the role associated with this notebook instnace to have \"s3:PutObject\" permissions and try this validation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and upload a sample test script.\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\"test_upload/test.txt\").upload_file('test_data/upload-test-file.txt')\n",
    "print(\"Success! You are all set to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Upload training data to S3 bucket.\n",
    "This training data will be used by the triggered model re-training job later in Section 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(\"train/\" + MOVIE_RECOMMENDATION_TRAIN_DATA).upload_file(LOCAL_DATA_DIR + '/' + MOVIE_RECOMMENDATION_TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Deploy pre-trained movie recommendation model with model data capture enabled <a id='deploy'></a>\n",
    "\n",
    "In this section, we will upload the pretrained model to the S3 bucket, create Amazon Sagemaker Model, configure Amazon SageMaker RealTime Endpoint with 'DataCapture' enabled and finally create the realtime endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Upload the pre-trained model to S3\n",
    "\n",
    "This code uploads a pre-trained XGBoost movie recommendation model that is ready for you to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy model to S3 bucket.\n",
    "def copy_model_to_s3(model_name):\n",
    "    key = prefix + \"/\" + model_name\n",
    "    with open(LOCAL_MODELS_DIR+'/'+model_name, 'rb') as file_obj:\n",
    "        print(\"Uploading \", file_obj , \" to bucket \",bucket, \" as \" , key)\n",
    "        boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy movie recommendation model to S3\n",
    "copy_model_to_s3(MOVIE_RECOMMENDATION_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Create SageMaker Model entity\n",
    "\n",
    "This step creates an Amazon SageMaker model from the movie recommendations model file previously uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')\n",
    "\n",
    "model_name = \"MoviePredictions-EndpointDataCaptureModel-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "model_url = 'https://{}.s3-{}.amazonaws.com/{}/{}'.format(bucket, region, prefix,MOVIE_RECOMMENDATION_MODEL)\n",
    "\n",
    "print (model_url)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_url,\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Create Endpoint Configuration\n",
    "\n",
    "This step is required to deploy an Amazon SageMaker model on a realtime endpoint.\n",
    "\n",
    "To enable data capture for monitoring the model data quality, you specify the new capture option called \"DataCaptureConfig\". You can capture the request payload, the response payload or both with this configuration. Captured data is persisted in the S3 bucket location YOU specify and have complete control over.  This allows you to version this data, secure it with IAM policies, encryption according to your needs. \n",
    "\n",
    "The data capture is supported at the endpoint configuration level and applies to all variants. The captured data is stored in a json format. The comments in the cell below highlight the new API parameters for data capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "data_capture_sub_folder = \"datacapture-xgboost-movie-recommendations\"\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_sub_folder)\n",
    "\n",
    "print(\"Capture path:\"+ s3_capture_upload_path)\n",
    "\n",
    "data_capture_configuration = {\n",
    "    \"EnableCapture\": True, # flag turns data capture on and off\n",
    "    \"InitialSamplingPercentage\": 90, # sampling rate to capture data. max is 100%\n",
    "    \"DestinationS3Uri\": s3_capture_upload_path, # s3 location where captured data is saved\n",
    "    \"CaptureOptions\": [\n",
    "        {\n",
    "            \"CaptureMode\": \"Output\" # The type of capture this option enables. Values can be: [Output/Input]\n",
    "        },\n",
    "        {\n",
    "            \"CaptureMode\": \"Input\" # The type of capture this option enables. Values can be: [Output/Input]\n",
    "        }\n",
    "    ],\n",
    "    \"CaptureContentTypeHeader\": {\n",
    "       \"CsvContentTypes\": [\"text/csv\"], # headers which should signal to decode the payload into CSV format \n",
    "       \"JsonContentTypes\": [\"application/json\"] # headers which should signal to decode the payload into JSON format \n",
    "     }\n",
    "}\n",
    "\n",
    "endpoint_config_name = 'XGBoost-MovieRec-DataCaptureEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m5.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'InitialVariantWeight':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTrafficVariant'\n",
    "    }],\n",
    "    DataCaptureConfig = data_capture_configuration) # This is where the new capture options are applied\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Create Endpoint\n",
    "This step uses the endpoint configuration specified above to create an endpoint. This takes a few minutes (approximately 10 minutes) to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'XGBoost-RW-MovieRec-DataCaptureEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Execute movie predictions and analyze data captured from the deployed movie recommendation model <a id='analyze-data-captured'></a>\n",
    "\n",
    "In this section, we will invoke the enpoint created above using test data.  Since we already enabled 'datacapture' on the endpoint, the request payload and response along with additional metadata will be saved in the S3 location you have specified.  We will examine the data captured in the S3 bucket.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  Invoke the Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <font color='red'> Using these endpoint for testing to iterate quickly.  TODO : Delete in final version. </font>\n",
    "#endpoint_name='XGBoost-RW-MovieRec-DataCaptureEndpoint-2019-12-02-09-48-07'\n",
    "#arn:aws:sagemaker:us-west-2:555360056434:endpoint/xgboost-rw-movierec-datacaptureendpoint-2019-12-02-09-48-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv(LOCAL_DATA_DIR+\"/\"+MOVIE_DATA, delimiter ='|', encoding='latin-1')\n",
    "\n",
    "movie_df.columns = [\"movie id\", \"movie title\", \"release date\", \"video release date\",\n",
    "              \"IMDb URL\", \"unknown\", \"Action\", \"Adventure\", \"Animation\",\n",
    "              \"Children's\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Fantasy\",\n",
    "              \"Film-Noir\",\"Horror\",\"Musical\",\"Mystery\", \"Romance\",\"Sci-Fi\",\n",
    "              \"Thriller\",\"War\",\"Western\"]\n",
    "\n",
    "#movie_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "def get_recommendations_for_user(user_id, show_predictions):\n",
    "    predictions_for_user = str(user_id)\n",
    "    predictions = []\n",
    "    \n",
    "    with open(LOCAL_DATA_DIR+\"/\"+'movielens_users_items_for_predictions.csv', 'r') as f:\n",
    "        contents = f.readlines() \n",
    "    \n",
    "    for i in range(0, len(contents) - 1):\n",
    "        line = contents[i]\n",
    "        split_data = line.split(',')\n",
    "        #Remove the original rating value from data used for prediction\n",
    "        original_value = split_data.pop(0)\n",
    "        original_value = split_data.pop(0)\n",
    "        #print('original rating ', original_value)\n",
    "\n",
    "        user = split_data[0]\n",
    "        item = split_data[1]\n",
    "        #print('Predicting rating for User ', user, 'for item ', item)\n",
    "\n",
    "        if (user == predictions_for_user) : \n",
    "\n",
    "            payload = ','.join(split_data)\n",
    "\n",
    "            response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                                  ContentType='text/csv', \n",
    "                                                  Body=payload)\n",
    "            prediction = response['Body'].read().decode('utf-8')\n",
    "\n",
    "            predictions.append([item, prediction])\n",
    "\n",
    "            #print(\"Original Value \", original_value , \"Prediction : \", float(prediction))\n",
    "\n",
    "    if show_predictions:        \n",
    "        sorted_predcitions =    sorted(predictions, key = lambda x: x[1], reverse=True)     \n",
    "\n",
    "        ## Let's show only the top 10 recommendations\n",
    "        recommendations = sorted_predcitions[0:9]\n",
    "\n",
    "        print(\"Recommended movies for user with id : \", predictions_for_user)\n",
    "        for rec in recommendations: \n",
    "            #print(\"rec is \", type(rec))\n",
    "            movie_id = int(rec[0])\n",
    "            #print(\"recommended_movie_item \", movie_id)\n",
    "            movie_match = movie_df.loc[movie_df['movie id'] == movie_id]\n",
    "            movie_titile = movie_match['movie title'].values[0]\n",
    "            print(\"\\t\", movie_match['movie title'].values[0] )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get movie recommendations for a couple of users.\n",
    "user_ids = [100, 235]\n",
    "\n",
    "for user_id in user_ids:\n",
    "    get_recommendations_for_user(user_id, True)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 View Captured Data : List files\n",
    "\n",
    "Now let's list the data capture files stored in S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the s3 path is:\n",
    "\n",
    "s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=data_capture_sub_folder)\n",
    "print(\"Looking for captured files in : \" , data_capture_sub_folder)\n",
    "\n",
    "while(result.get('Contents') == None):\n",
    "    time.sleep(30)\n",
    "    print(\"Looking for captured files in : \" , data_capture_sub_folder)\n",
    "    result = s3_client.list_objects(Bucket=bucket, Prefix=data_capture_sub_folder)\n",
    "    \n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "current_hour_prefix=strftime(\"%Y/%m/%d/%H\", gmtime())\n",
    "\n",
    "current_hour=strftime(\"%H\", gmtime())\n",
    "print(\"current_hour  : \",current_hour)\n",
    "previous_hour=int(current_hour) - 1\n",
    "print(\"previous_hour  : \",previous_hour)\n",
    "\n",
    "print(\"current_hour_prefix  : \", current_hour_prefix)\n",
    "previous_hour_prefix=strftime(\"%Y/%m/%d/\"+str(previous_hour), gmtime())\n",
    "print(\"previous_hour_prefix  : \", previous_hour_prefix)\n",
    "\n",
    "  \n",
    "prefix_for_endpoint_captured_files = data_capture_sub_folder+\"/\"+endpoint_name+\"/AllTrafficVariant/\"\n",
    "prefix_for_endpoint_captured_files_current_hour = prefix_for_endpoint_captured_files+current_hour_prefix\n",
    "prefix_for_endpoint_captured_files_previous_hour = prefix_for_endpoint_captured_files+previous_hour_prefix\n",
    "\n",
    "print(\"Checking for captured files for current hour at prefix \", prefix_for_endpoint_captured_files_current_hour)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=prefix_for_endpoint_captured_files_current_hour)\n",
    "\n",
    "files_found=[]\n",
    "\n",
    "if result.get('Contents'):\n",
    "    print(\"\\tFound Capture Files:\")\n",
    "    \n",
    "    for capture_file in result.get('Contents'):\n",
    "        print(\"capture file \", capture_file)\n",
    "        key_name_parts = capture_file['Key'].split(\"/\")\n",
    "        file_name = key_name_parts[-1]\n",
    "        print(\"File name \", file_name)\n",
    "        files_found.append(file_name)\n",
    "\n",
    "    capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "    print(\"\\n \".join(capture_files))\n",
    "\n",
    "print(\"Checking for captured files for previous hour at prefix \", prefix_for_endpoint_captured_files_previous_hour)\n",
    "\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=prefix_for_endpoint_captured_files_previous_hour)\n",
    "\n",
    "if result.get('Contents'):\n",
    "    print(\"\\tFound Capture Files:\")\n",
    "    capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "else:\n",
    "    print(\"\\tDid not find any Capture Files:\")\n",
    "    \n",
    "    #Let's copy from the current hour\n",
    "    ##Copy the file with key \n",
    "    for file in files_found:\n",
    "        key = prefix_for_endpoint_captured_files_current_hour+\"/\"+file\n",
    "        print(\"key is \", key)\n",
    "        copy_source = {'Bucket': bucket, 'Key': key }\n",
    "        print(\"Copying \", key , \" to \", prefix_for_endpoint_captured_files_previous_hour+\"/\"+file)\n",
    "        s3_client.copy_object(CopySource=copy_source, \n",
    "                              Bucket=bucket,\n",
    "                              Key=prefix_for_endpoint_captured_files_previous_hour+\"/\"+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 View Captured Data : View captured file content\n",
    "\n",
    "Next, let's view the contents of a single capture file. Here you should see all the data captured in a json-line formatted file. For each inference made against the real time endpoint, a single line is captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "\n",
    "print(capture_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 View Captured Data : View single line captured\n",
    "\n",
    "Finally, let's view the contents of a single line. For each inference endpoint input, endpoint output and event meta data is captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 – Generate baseline statistics and constraints for continous model monitoring <a id='generate-baseline'></a>\n",
    "\n",
    "In this section, we will ask Amazon SageMaker to  suggest a set of baseline constraints and generate descriptive statistics to explain the data. We will review the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Upload baseline data to S3\n",
    "Training dataset which was used for training the xgboost movie recommendation model is provided with this notebook.  We will first upload this training dataset as baseline  to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over the training dataset to S3 \n",
    "baseline_prefix = prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(bucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(bucket, baseline_results_prefix)\n",
    "print(baseline_data_uri)\n",
    "print(baseline_results_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data_file = open(LOCAL_DATA_DIR+\"/\"+MOVIE_RECOMMENDATION_BASELINE_DATA, 'rb')\n",
    "s3_key = os.path.join(baseline_prefix, 'data', 'baseline.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_key).upload_fileobj(baseline_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Generate suggested constraints with baseline/training dataset using a Processing job\n",
    "\n",
    "Let's ask SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data, using the uploaded baseline dataset.  For this, we will create a Processing job\n",
    "\n",
    "(This step will take approximately 6 min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "from processingjob_wrapper import ProcessingJob\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'MOVIE-REC-baseline-xgb-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "processing_job = ProcessingJob(sm_client, role).create(job_name, baseline_data_uri, baseline_results_uri)\n",
    "\n",
    "resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "status = resp['ProcessingJobStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='InProgress':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "    status = resp['ProcessingJobStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "if status=='Failed':\n",
    "    print(resp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Explore the generated constraints and statistics\n",
    "\n",
    "<font color='red'> TODO : Render constraints and stats in tabular way.  Did this, but may be not showing all features?? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using this for quickly iterate : MOVIE-REC-baseline-xgb-model-monitor-2019-12-02-09-57-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))\n",
    "\n",
    "## TODO : Same method defined multiple times.  Consolidate.\n",
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_file = get_obj_body(baseline_results_prefix+'/constraints.json')\n",
    "#print(constraints_file[:4000])\n",
    "print(constraints_file)\n",
    "##Point out the various inferred_type.  Fractional, Integral, String (ZipCode),\n",
    "##TODO : Why are the genres treated as fractional?  \n",
    "##TODO : Would making rating fractional makes more sense?\n",
    "##TODO : What are all available inferred_types available and can we show them all?\n",
    "##TODO : set age level constraint ?  Gender level constraint\n",
    "##TODO : ADD explanation of the constraints\n",
    "###TODO : Better rendering from the pysdk sample notebook.  Can we render as histograms as well??\n",
    "\n",
    "##Lets add details of these monitoring config settings.  Arun to forward the document.\n",
    "##Featurre level override is also available.\n",
    "\n",
    "##Base line 20 cols, but inference has 21 cols -- extra column check (what happens if we send 1 less column)\n",
    "## How about if one col is null ?? Completeness constraint.\n",
    "\n",
    "\n",
    "## Good to include in deck\n",
    "## Other Drift examples : loan applicaiton model.  loan approved/not; how much.  csutomers start using mobile and apply high amount.\n",
    "## could have caught in the drift.\n",
    "\n",
    "## DS retrained model based on a pivot in a single dimension.  now data formats coming in new inference.\n",
    "## new applicaiton inference traffic formats.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results examine the monitoring constraints section at the end of the constriants file.  Note that these constraints are suggestions from Amazon SageMaker.  You can choose to override these values.  You can even update this file to include feature level constraint override."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : ADD explanation of the statistics\n",
    "statistics_file = get_obj_body(baseline_results_prefix+'/statistics.json')\n",
    "print(statistics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Render the important statistics that you want to observe in a tabular format\n",
    "#import monitor_render_utils as mu\n",
    "\n",
    "#feature_baselines = mu.get_features(statistics_json)\n",
    "#mu.show_distributions(feature_baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Render constraints in a tabular format.\n",
    "#constraints_json = json.loads(constraints_file)\n",
    "#schema_df = pd.io.json.json_normalize(constraints_json['features'])\n",
    "#schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 – Monitor and analyze model for data drift <a id='analyze-data-drift'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have captured data based on inferences to the deployed realtime endpoint and explored the baseline data contraints and statistics.  In this section, we will take this a step forward and configure continuous monitoring using 'Monitoring Schedules'.  We will further use results of various monitoring schedule executions to analyse inference data quality and detect data drift.\n",
    "\n",
    "**Note** that the datacapture capability can be used independent of model monitoring.  Gaining understanding of the baseline/inference data and the model can benefit is your usecase regardless of whether you choose to continously monitor the deployed models in production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Create a schedule\n",
    "Let's create a Monitoring schedule for the previously created Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first copy over some test scripts to the S3 bucket so that they can be used for pre and post processing\n",
    "## HOW SHOULD WE USE preprocessor /postprocessor FOR THE WORKSHOP??\n",
    "## IS This for baseline data? or inference data?\n",
    "\n",
    "## OPTIONAL /ADVANCED SECTION \n",
    "#multiple schedules\n",
    "#BYOC\n",
    "#infereance data capture --> should match the baseline\n",
    "#independent of the data capture\n",
    "#Prebuilt rules assume certain data structure/format.\n",
    "\n",
    "##TODO : WE NEED TO DELETE THIS?  ACTUALLY, MOVE TO AN ADVANCED SECTION\n",
    "\n",
    "##NEED THESE FILES FOR MONITORING SCHEDULE JOB TO WORK\n",
    "\n",
    "## Pre processing -- \n",
    "## Post processing -- write additonal file as are report?\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(code_prefix+\"/preprocessor.py\").upload_file('preprocessor.py')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(code_prefix+\"/postprocessor.py\").upload_file('postprocessor.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schedule_wrapper import MonitoringSchedule\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# MonitoringSchedule is just a python helper to hide the large CreateMonitoringSchedule input payload. You can find it\n",
    "# in scheduler_wrapper.py in this package\n",
    "\n",
    "ms = MonitoringSchedule(sm_client, role)\n",
    "mon_schedule_name = 'REC-SM-xgb-movie-rec-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "schedule = ms.create(mon_schedule_name, endpoint_name, s3_report_path, \n",
    "                     record_preprocessor_source_uri=s3_code_preprocessor_uri, \n",
    "                     post_analytics_source_uri=s3_code_postprocessor_uri,\n",
    "                     baseline_statistics_uri=baseline_results_uri + '/statistics.json',\n",
    "                     baseline_constraints_uri=baseline_results_uri+ '/constraints.json')\n",
    "\n",
    "\n",
    "schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = sm_client.describe_monitoring_schedule( MonitoringScheduleName=mon_schedule_name)\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Generate inference traffic \n",
    "\n",
    "Start generating some inference traffic. The block below kicks off a thread to send traffic to the created endpoint. Note that you need to stop the kernel to terminate this thread. Just having this here so that it can continue to generate traffic. If there is no traffic, the monitoring jobs will start to fail later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "user_ids = [100, 235, 40, 500, 300, 500, 600, 489, 293, 95]\n",
    "\n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        #invoke_endpoint(endpoint_name, LOCAL_DATA_DIR+\"/\"+MOVIE_RECOMMENDATION_TEST_DATA, runtime_client)\n",
    "        for user_id in user_ids:\n",
    "            get_recommendations_for_user(user_id, False)\n",
    "        \n",
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Describe and inspect the schedule.  \n",
    "Once you describe, see MonitoringScheduleStatus changes to Scheduled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = sm_client.describe_monitoring_schedule( MonitoringScheduleName=mon_schedule_name)\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 List executions \n",
    "List executions\n",
    "Once the schedule is scheduled, it will kick of jobs at specified intervals. Here we are listing the latest 5 executions. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait till you cross the hour boundary (in UTC) to see executions kick off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "mon_executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you may see the first few executions of the monitoring schedule fail due to data unavailability in the S3 bucket.  This is because the execution is looking for data captured in the previous hour.  \n",
    "\n",
    "Once the execution starts looking for the data in the current hour, you will start seeing execution completions.\n",
    "\n",
    "##TODO : For now copying files to the right folder.  But should fix this.\n",
    "\n",
    "Examples of drift.  Why is it important to monitor this?  What is the imapact.  -- Deck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Wait for first execution of the monitoring schedule\n",
    "\n",
    "First execution of the monitoring schedule will happen ON the hour mark.  This cell waits for the first execution.\n",
    "\n",
    "**Note** Since this cell execution could potentially take a long time waiting, leave this cell running and explore the rest of this section.  For your convinience, sample outputs for next few cells are included for you to examine.\n",
    "Please DO NOT close this notebook till executions of this cell is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "latest_execution=None ##TODO : Should we be capturing first execution instead of the latest??\n",
    "\n",
    "# Wait till an execution occurs\n",
    "while not mon_executions['MonitoringExecutionSummaries']:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Wait for completion of first monitoring schedule execution\n",
    "\n",
    "You should see \"Processing Job Status: InProgress\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "\n",
    "if len(mon_executions['MonitoringExecutionSummaries']) == 1: \n",
    "    execution = mon_executions['MonitoringExecutionSummaries'][0]\n",
    "    while True:\n",
    "        if execution['ProcessingJobArn']:\n",
    "            job_name = execution['ProcessingJobArn'].split('/')[1]    \n",
    "            resp = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "            status = resp['ProcessingJobStatus']\n",
    "            print(\"Processing Job Status: \" + status)\n",
    "            if status != 'InProgress':\n",
    "                break\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7 Get the latest execution details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the latest execution details \n",
    "mon_executions = sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n",
    "\n",
    "for execution_summary in mon_executions['MonitoringExecutionSummaries']:\n",
    "    print(\"ProcessingJob: {}\".format(execution_summary['ProcessingJobArn'].split('/')[1]))\n",
    "    print('MonitoringExecutionStatus: {} \\n'.format(execution_summary['MonitoringExecutionStatus']))\n",
    "    print(\"execution_summary is \" , execution_summary)\n",
    "    print(\"latest_executions is \" , latest_execution)    \n",
    "    if not latest_execution:\n",
    "        exec_status = execution_summary['MonitoringExecutionStatus']\n",
    "        print(\"exec_status is \" , exec_status)    \n",
    "        if  exec_status == 'Completed' or exec_status == 'Failed' or exec_status == 'CompletedWithViolations':\n",
    "            latest_execution = execution_summary\n",
    "            \n",
    "print(\"latest_executions is \" , latest_execution)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.list_monitoring_executions(MonitoringScheduleName=mon_schedule_name.lower(), MaxResults=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE OUTPUT\n",
    "\n",
    "Here are the possible terminal states for monitoring schedule execution.\n",
    "\n",
    "* Completed - Monitoring execution is completed and no issues were found in the violations report\n",
    "\n",
    "* CompletedWithViolations - Monitoring execution is completed, but constraint violations were detected\n",
    "\n",
    "* Failed - Monitoring execution failed, may be due to client error (say role issues) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened.\n",
    "\n",
    "\n",
    "You can see that atleast one of the monitoring schedule executions is in 'CompletedWithViolations'. You will explore the violations in the next few cells.\n",
    "\n",
    "<div style=\"background-color:orange; text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "<img src=\"images/MonitoringExecutionsSummary.png\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.8 Inspect a specific execution (latest execution here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, we picked up the latest Completed/Failed scheduled execution. First let's see where are the results are stored in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest_execution:\n",
    "    job_name=latest_execution['ProcessingJobArn'].split('/')[1]\n",
    "    job_status=latest_execution['MonitoringExecutionStatus']\n",
    "    desc_analytics_job_result=sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "    \n",
    "    if job_status == 'Completed' or job_status == 'CompletedWithViolations':\n",
    "        report_uri=desc_analytics_job_result['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "        print('Report Uri: {}'.format(report_uri))\n",
    "    else:\n",
    "        print('Job failed, todo: print failure reason and more details..')\n",
    "else:\n",
    "    print(\"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE OUTPUT : REPORT FILE LOCATION\n",
    "\n",
    "<div style=\"background-color:orange; text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "<img src=\"images/ReportURI.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.9 List the generated reports\n",
    "\n",
    "Next, let's look at the report file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip('/')\n",
    "print('Report bucket: {}'.format(report_bucket))\n",
    "print('Report key: {}'.format(report_key))\n",
    "\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE OUTPUT : REPORTS FILES\n",
    "\n",
    "<div style=\"background-color:orange; text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "<img src=\"images/ReportFiles.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.10 Violations report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any violations observed in the inference traffic compared to the baseline data, it will be generated here. Let's list the violations.\n",
    "\n",
    "<font color='red'> TODO : Add explanation to one or two features : 'Age' and 'Gender'? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "violations_file = get_obj_body(report_key+'/constraint_violations.json')\n",
    "print(violations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE OUTPUT : VIOLATIONS REPORT\n",
    "\n",
    "<div style=\"background-color:orange; text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "<img src=\"images/Violations.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 - Retrigger Training <a id='retrigger-training'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to capture the violations, in this section we can go a step further and automatically trigger retraining of the movie recommendation model.  We rely on Amazon CloudWatch metrics and alerts for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Get the right SNS topic to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the SNS topics available.\n",
    "Make note of the topic ARN that consists of 'RetrainSNSTopic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sns')\n",
    "\n",
    "client.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_notifications_topic='<<REPLACE_SNS_TOPIC_ARN>>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Create Cloud Watch Alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Threshold for cloudwatch is independent of violations threshold.\n",
    "##Show the sequence of 0.1,0.2, but only trigger at 0.5\n",
    "##Email notification -- Talk about this.  Add to additions or optional section.\n",
    "\n",
    "cw_client = boto3.Session().client('cloudwatch')\n",
    "\n",
    "alarm_name='BASELINE_DRIFT_FEATURE_AGE'\n",
    "alarm_desc='Trigger an cloudwatch alarm when the feature age drifts away from the baseline'\n",
    "feature_age_drift_threshold=0.1\n",
    "metric_name='feature_baseline_drift_Age'\n",
    "namespace='aws/sagemaker/Endpoints/data-metrics'\n",
    "\n",
    "endpoint_name=endpoint_name\n",
    "monitoring_schedule_name=mon_schedule_name\n",
    "\n",
    "cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    AlarmDescription=alarm_desc,\n",
    "    ActionsEnabled=True,\n",
    "    AlarmActions=[sns_notifications_topic],\n",
    "    MetricName=metric_name,\n",
    "    Namespace=namespace,\n",
    "    Statistic='Average',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'Endpoint',\n",
    "            'Value': endpoint_name\n",
    "        },\n",
    "        {\n",
    "            'Name': 'MonitoringSchedule',\n",
    "            'Value': monitoring_schedule_name\n",
    "        }\n",
    "    ],\n",
    "    Period=600,\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=feature_age_drift_threshold,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='breaching'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation - 1\n",
    "\n",
    "In a few minutes, you should see a cloudwatch alarm created. The alarm will first be in \"Insufficient Data\" state and moves into \"Alert\" state.  This can be verified in the CloudWatch console\n",
    "\n",
    "![title](images/CloudWatchAlarm_InsufficientData.png)\n",
    "\n",
    "![title](images/CloudWatchAlarm_Alert.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation - 2\n",
    "\n",
    "In a few minutes, you should also see a new SageMaker model training job is triggered.  This can be verified in the SageMaker Console\n",
    "\n",
    "\n",
    "![title](images/ModelRetraining.png)\n",
    "\n",
    "<font color='red'> TODO : EXPLAIN.  What is the training data used for retraining?  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Cleanup : Delete the Resources <a id='cleanup'></a>\n",
    "\n",
    "You can keep your Endpoint running to continue capturing data. If you do not plan to collect more data or use this endpoint further, you should delete the endpoint to avoid incurring additional charges. Note that deleting your endpoint does not delete the data that was captured during the model invocaations. That data is persisted in S3 until you delete it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_schedule_name='REC-SM-xgb-movie-rec-model-monitor-schedule-2019-12-01-21-49-45'\n",
    "#sm_client.list_monitoring_schedules()\n",
    "sm_client.delete_monitoring_schedule(MonitoringScheduleName=mon_schedule_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
