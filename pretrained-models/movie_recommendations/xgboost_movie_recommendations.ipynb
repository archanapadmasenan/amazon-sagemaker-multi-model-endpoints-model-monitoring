{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendations with XGBoost\n",
    "_**Using Gradient Boosted Trees to Provide Movie Recommendations**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Compile](#Compile)\n",
    "1. [Host](#Host)\n",
    "  1. [Evaluate](#Evaluate)\n",
    "  1. [Relative cost of errors](#Relative-cost-of-errors)\n",
    "1. [Extensions](#Extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "\n",
    "TODO\n",
    "\n",
    "This notebook will NOT be part of the workshop.  This one is used to pretrain the xgboost movie recommendation model.  Trained model should be uploaded to the correct S3 bucket to be used for deployment\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge TODO : Check notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-us-west-2-555360056434'  ##TODO : Change this to session bucket\n",
    "prefix = 'sagemaker/recommendations-xgboost-movie'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the Python libraries we'll need for the remainder of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "Explain Movie Lens Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Is the movie lens data with all feature prepped??\n",
    "feature_data_prepared = False\n",
    "\n",
    "if (os.path.exists('ml-100k/movielens_data_allfeatures.csv')):\n",
    "    feature_data_prepared = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : Make this conditional.  Only need this the first time the data is being prepared\n",
    "\n",
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine data from multiple files to create training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : Make this conditional.  Only need this the first time the data is being prepared\n",
    "\n",
    "\n",
    "## Start by reading ua.base into a dataframe TODO : Should this be u.user??\n",
    "df = pd.read_csv('ml-100k/u.data', header=None, delimiter = '\\t')\n",
    "df.columns = [\"User\", \"Item\", \"Rating\", \"TimeStamp\"]\n",
    "\n",
    "len(df)\n",
    "print( df[\"User\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : Make this conditional.  Only need this the first time the data is being prepared\n",
    "\n",
    "## Now get the additional columns for user gender, age, occupation, Zipcode\n",
    "df_user = pd.read_csv('ml-100k/u.user', header=None, delimiter = '|')\n",
    "df_user.columns = [\"User\", \"Age\", \"Gender\", \"Occupation\", \"Zipcode\"]\n",
    "\n",
    "len(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Combine the two dataframes to get the complete data set.\n",
    "#Iterate through the dataframe\n",
    "\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    #print(\"row is \", type(row), \" : \", row)\n",
    "    user_id = row['User']\n",
    "    \n",
    "    #print(\"find a match for user_id \", user_id)\n",
    "    ##For this user_id get gender, occupation and zipcode\n",
    "    match = df_user.loc[df_user['User'] == user_id]\n",
    "    user_gender = match['Gender'].values[0]\n",
    "    user_occupation = match['Occupation'].values[0]\n",
    "    user_zipcode = match['Zipcode'].values[0]\n",
    "    \n",
    "    df.at[i,\"Gender\"] = user_gender\n",
    "    df.at[i,\"Occupation\"] = user_occupation    \n",
    "    df.at[i,\"Zip Code\"] = user_zipcode \n",
    " \n",
    "print(\"After update\")\n",
    "print(df[:100])\n",
    "\n",
    "df.to_csv(\"ml-100k/movielens_data_allfeatures.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess feature of the movie lens data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lens_data_df = pd.read_csv('ml-100k/movielens_data_allfeatures.csv')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "movie_lens_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Remove the unnamed:0 column\n",
    "movie_lens_data_df.drop(['Unnamed: 0'],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode categorial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode \"Gender\" \n",
    "movie_lens_data_df = pd.concat([movie_lens_data_df,pd.get_dummies(movie_lens_data_df['Gender'], prefix='Gender')],axis=1)\n",
    "movie_lens_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Drop the original feature, since it is not needed anymore.\n",
    "movie_lens_data_df.drop(['Gender'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the 'Occupation' attribute \n",
    "movie_lens_data_df = pd.concat([movie_lens_data_df,pd.get_dummies(movie_lens_data_df['Occupation'], prefix='Occupation')],axis=1)\n",
    "movie_lens_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Drop the original feature, since it is not needed anymore.\n",
    "movie_lens_data_df.drop(['Occupation'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##For SageMaker XGBoost, the predictor variable should be the first column and there should be no headers in the file.\n",
    "##So move the 'Rating' colum to the begining of the dataframe.\n",
    "rating = movie_lens_data_df['Rating']\n",
    "movie_lens_data_df.drop(labels=['Rating'], axis=1,inplace = True)\n",
    "movie_lens_data_df.insert(0, 'Rating', rating)\n",
    "movie_lens_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check the columns after all the processing.\n",
    "movie_lens_data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data : TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's split the data into training, validation, and test sets.  This will help prevent us from overfitting the model, and allow us to test the models accuracy on data it hasn't already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pick up here again.\n",
    "\n",
    "train_data, validation_data, test_data = np.split(movie_lens_data_df.sample(frac=1, random_state=1729), [int(0.7 * len(movie_lens_data_df)), int(0.9 * len(movie_lens_data_df))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "\n",
    "\n",
    "print(\"Number of training samples : \" , len(train_data))\n",
    "print(\"Number of validation samples : \" , len(validation_data))\n",
    "print(\"Number of test samples : \" , len(test_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload these files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=6,\n",
    "                        eta=0.2,\n",
    "                        gamma=5,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.9,\n",
    "                        silent=0,\n",
    "                        objective='reg:linear',\n",
    "                        num_round=60)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Host\n",
    "\n",
    "Now that we've trained the algorithm, let's create a model and deploy it to a hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batchs to CSV string payloads\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_data type is \", type(test_data))\n",
    "\n",
    "ratings = test_data['Rating']\n",
    "\n",
    "print(\"ratings type \", type(ratings))\n",
    "\n",
    "test_data.drop('Rating', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Indice causing errors : 48,75,95.134\n",
    "test_data_matrix = test_data.as_matrix()\n",
    "\n",
    "#Try removing these indices : TODO\n",
    "\n",
    "print(\"test_data_matrix type is \", type(test_data_matrix), \" shape \", test_data_matrix.shape)\n",
    "\n",
    "test_data_matrix_subset = test_data_matrix[:10]\n",
    "\n",
    "predictions=[]\n",
    "\n",
    "for i in range(0, 20):\n",
    "    predicted_value = xgb_predictor.predict(test_data_matrix[i])\n",
    "    predictions.append(predicted_value)\n",
    "    print(\"predicted value \", predicted_value)\n",
    "    \n",
    "print(\"Number of predictions \", len(predictions))\n",
    "print(\"Number of original ratings \", len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compare with the original values \n",
    "for i in range(0, 20):\n",
    "    #Prediction returned is a byte array.  Convert this to float to compare with the original\n",
    "    prediction = float(predictions[i].decode())  \n",
    "    print(\"predicted value \", prediction, \" original value \", ratings.values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TODO : Show some metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clean-up\n",
    "\n",
    "If you're ready to be done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
